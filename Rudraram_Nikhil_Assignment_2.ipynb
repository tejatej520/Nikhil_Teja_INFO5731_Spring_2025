{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejatej520/Nikhil_Teja_INFO5731_Spring_2025/blob/main/Rudraram_Nikhil_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Monday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (25 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "\n",
        "(3) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(4) Collect all the information of the 904 narrators in the Densho Digital Repository.\n",
        "\n",
        "(5)**Collect a total of 10000 reviews** of the top 100 most popular software from G2 and Capterra.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfa8fecb-d8fb-438a-bfc6-ea1f6451494a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Request failed with status code 429: {\"message\": \"Too Many Requests. Please wait and try again or apply for a key for higher rate limits. https://www.semanticscholar.org/product/api#api-key-form\", \"code\": \"429\"}\n",
            "Saved 0 papers to abstract_papers.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# I choosed this key wird to get researc papers\n",
        "query = \"Artifical Intellgence\"\n",
        "\n",
        "url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "\n",
        "# Set query parameters\n",
        "query_params = { \"query\": query, \"offset\": 0, \"limit\": 100, \"fields\": \"paperId,title,abstract\" }\n",
        "\n",
        "\n",
        "# List to hold all paper results\n",
        "papers_list = []\n",
        "\n",
        "\n",
        "while len(papers_list) < 10:\n",
        "    response = requests.get(url, params=query_params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        response_data = response.json()\n",
        "        papers = response_data.get(\"data\", [])\n",
        "\n",
        "        if not papers:\n",
        "            print(\"No more papers returned from the API.\")\n",
        "            break\n",
        "\n",
        "        for paper in papers:\n",
        "            papers_list.append({ \"paperId\": paper.get(\"paperId\"), \"title\": paper.get(\"title\"), \"abstract\": paper.get(\"abstract\") })\n",
        "            if len(papers_list) >= 1000:\n",
        "                break\n",
        "\n",
        "\n",
        "        query_params['offset'] += query_params['limit']\n",
        "\n",
        "\n",
        "        if len(papers_list) < 1000:\n",
        "            time.sleep(5)\n",
        "    else:\n",
        "        print(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
        "        break\n",
        "\n",
        "\n",
        "csv_filename = \"abstract_papers.csv\"\n",
        "with open(csv_filename, mode=\"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
        "    fieldnames = [\"paperId\", \"title\", \"abstract\"]\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for paper in papers_list:\n",
        "        writer.writerow(paper)\n",
        "\n",
        "print(f\"Saved {len(papers_list)} papers to {csv_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (15 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ce137ca-a3d1-451c-9007-441fb6bda756"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      paperId  \\\n",
            "0    31fc27bad541386fcbb9c6e1bceb614285055135   \n",
            "1    4842a22bb8ec6ce57135a7b298a2a260d1ac8d90   \n",
            "2    9de4bb94be3410228b3880296de34ff09a883898   \n",
            "3    9f5ed051dc2de7d655565212de4b424688bc8f04   \n",
            "4    8f9340acd5e4141db234d6ac75d947e5c5ccd5bd   \n",
            "..                                        ...   \n",
            "295  fc31d42b5386eba58833e69d09e51197db4ce80c   \n",
            "296  f9f8b390cc41f6da0787848b3c11ca329774c6d3   \n",
            "297  405c9a82bd27cbb6979a50b33fd3d0482bc82624   \n",
            "298  c07ab857fdbb5641ef0ac1437c2533b0ded2ac55   \n",
            "299  2337681c4cfcfbab22614618212f1f98c17087a1   \n",
            "\n",
            "                                                 title  \\\n",
            "0          ROLE OF ARTIFICAL INTELLIGENCE IN EDUCATION   \n",
            "1    Enhancing Operational Efficiency: Integrating ...   \n",
            "2    Black-box creativity and generative artifical ...   \n",
            "3    Artifical intelligence use in orthopedics: an ...   \n",
            "4    Artificial Intellgence in the Era of Precision...   \n",
            "..                                                 ...   \n",
            "295  Zero-order gratings used as an artificial dist...   \n",
            "296  COordinate Rotation DIgital Computer (CORDIC) ...   \n",
            "297  Hydrogenase in actinorhizal root nodules and r...   \n",
            "298  Global Motion Planning using a Laplacian Poten...   \n",
            "299  Emotional intelligence and leadership in organ...   \n",
            "\n",
            "                                              abstract  \n",
            "0    Now a days using of AI is became a part of edu...  \n",
            "1    With the rapid advancement of information tech...  \n",
            "2    The rapid rise of generative artificial intell...  \n",
            "3    Artificial intelligence (AI) is increasingly b...  \n",
            "4    Rapid-paced development and adaptability of ar...  \n",
            "..                                                 ...  \n",
            "295  Zero order gratings are periodic structures wi...  \n",
            "296                                                NaN  \n",
            "297  Hydrogenases were measured in intact actinorhi...  \n",
            "298  This paper describes a motion planning method ...  \n",
            "299  OF THE DISSERTATION EMOTIONAL INTELLGENCE AND ...  \n",
            "\n",
            "[300 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "# Write code for each of the sub parts with proper comments.\n",
        "import pandas as pd\n",
        "\n",
        "file_path = \"/content/abstract_papers.csv\"\n",
        "\n",
        "file_copy=pd.read_csv(file_path)\n",
        "\n",
        "df=pd.DataFrame(file_copy)\n",
        "\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download required NLTK data (if not already downloaded)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTxn8CSebkqk",
        "outputId": "b8c07272-9779-4ff9-e6f3-26c55049f17f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def final_clean_text(text):\n",
        "    if isinstance(text, str):\n",
        "        # Remove noise (special characters/punctuation)\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        # Remove numbers\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        # Lowercase all texts\n",
        "        text = text.lower()\n",
        "        # Remove stopwords\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "        # Stemming (optional: note that applying both stemming and lemmatization is not typical,\n",
        "        # but is done here per the requirement)\n",
        "        ps = PorterStemmer()\n",
        "        text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "        # Lemmatization\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "        return text\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "# Apply the cleaning function to the 'abstract' column and save the result in a new column 'clean_text'\n",
        "df['clean_text'] = df['abstract'].apply(final_clean_text)"
      ],
      "metadata": {
        "id": "0FhmoFURbhJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "HEntkTQccNLS",
        "outputId": "aad33552-f9d9-4cfc-ebfb-76613e627e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      paperId  \\\n",
              "0    31fc27bad541386fcbb9c6e1bceb614285055135   \n",
              "1    4842a22bb8ec6ce57135a7b298a2a260d1ac8d90   \n",
              "2    9de4bb94be3410228b3880296de34ff09a883898   \n",
              "3    9f5ed051dc2de7d655565212de4b424688bc8f04   \n",
              "4    8f9340acd5e4141db234d6ac75d947e5c5ccd5bd   \n",
              "..                                        ...   \n",
              "295  fc31d42b5386eba58833e69d09e51197db4ce80c   \n",
              "296  f9f8b390cc41f6da0787848b3c11ca329774c6d3   \n",
              "297  405c9a82bd27cbb6979a50b33fd3d0482bc82624   \n",
              "298  c07ab857fdbb5641ef0ac1437c2533b0ded2ac55   \n",
              "299  2337681c4cfcfbab22614618212f1f98c17087a1   \n",
              "\n",
              "                                                 title  \\\n",
              "0          ROLE OF ARTIFICAL INTELLIGENCE IN EDUCATION   \n",
              "1    Enhancing Operational Efficiency: Integrating ...   \n",
              "2    Black-box creativity and generative artifical ...   \n",
              "3    Artifical intelligence use in orthopedics: an ...   \n",
              "4    Artificial Intellgence in the Era of Precision...   \n",
              "..                                                 ...   \n",
              "295  Zero-order gratings used as an artificial dist...   \n",
              "296  COordinate Rotation DIgital Computer (CORDIC) ...   \n",
              "297  Hydrogenase in actinorhizal root nodules and r...   \n",
              "298  Global Motion Planning using a Laplacian Poten...   \n",
              "299  Emotional intelligence and leadership in organ...   \n",
              "\n",
              "                                              abstract  \\\n",
              "0    Now a days using of AI is became a part of edu...   \n",
              "1    With the rapid advancement of information tech...   \n",
              "2    The rapid rise of generative artificial intell...   \n",
              "3    Artificial intelligence (AI) is increasingly b...   \n",
              "4    Rapid-paced development and adaptability of ar...   \n",
              "..                                                 ...   \n",
              "295  Zero order gratings are periodic structures wi...   \n",
              "296                                                NaN   \n",
              "297  Hydrogenases were measured in intact actinorhi...   \n",
              "298  This paper describes a motion planning method ...   \n",
              "299  OF THE DISSERTATION EMOTIONAL INTELLGENCE AND ...   \n",
              "\n",
              "                                            clean_text  \n",
              "0    day use ai becam part educ differ method use e...  \n",
              "1    rapid advanc inform technolog busi intellig da...  \n",
              "2    rapid rise gener artifici intellig applic pose...  \n",
              "3    artifici intellig ai increasingli util orthope...  \n",
              "4    rapidpac develop adapt artifici intellig algor...  \n",
              "..                                                 ...  \n",
              "295  zero order grate period structur period small ...  \n",
              "296                                                NaN  \n",
              "297  hydrogenas measur intact actinorhiz root nodul...  \n",
              "298  paper describ motion plan method use artif pot...  \n",
              "299  dissert emot intellg leadership organ metaanal...  \n",
              "\n",
              "[300 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-943f3a17-46b7-4a09-8aa8-2f095189db7c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paperId</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>31fc27bad541386fcbb9c6e1bceb614285055135</td>\n",
              "      <td>ROLE OF ARTIFICAL INTELLIGENCE IN EDUCATION</td>\n",
              "      <td>Now a days using of AI is became a part of edu...</td>\n",
              "      <td>day use ai becam part educ differ method use e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4842a22bb8ec6ce57135a7b298a2a260d1ac8d90</td>\n",
              "      <td>Enhancing Operational Efficiency: Integrating ...</td>\n",
              "      <td>With the rapid advancement of information tech...</td>\n",
              "      <td>rapid advanc inform technolog busi intellig da...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9de4bb94be3410228b3880296de34ff09a883898</td>\n",
              "      <td>Black-box creativity and generative artifical ...</td>\n",
              "      <td>The rapid rise of generative artificial intell...</td>\n",
              "      <td>rapid rise gener artifici intellig applic pose...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9f5ed051dc2de7d655565212de4b424688bc8f04</td>\n",
              "      <td>Artifical intelligence use in orthopedics: an ...</td>\n",
              "      <td>Artificial intelligence (AI) is increasingly b...</td>\n",
              "      <td>artifici intellig ai increasingli util orthope...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8f9340acd5e4141db234d6ac75d947e5c5ccd5bd</td>\n",
              "      <td>Artificial Intellgence in the Era of Precision...</td>\n",
              "      <td>Rapid-paced development and adaptability of ar...</td>\n",
              "      <td>rapidpac develop adapt artifici intellig algor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>fc31d42b5386eba58833e69d09e51197db4ce80c</td>\n",
              "      <td>Zero-order gratings used as an artificial dist...</td>\n",
              "      <td>Zero order gratings are periodic structures wi...</td>\n",
              "      <td>zero order grate period structur period small ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>f9f8b390cc41f6da0787848b3c11ca329774c6d3</td>\n",
              "      <td>COordinate Rotation DIgital Computer (CORDIC) ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>405c9a82bd27cbb6979a50b33fd3d0482bc82624</td>\n",
              "      <td>Hydrogenase in actinorhizal root nodules and r...</td>\n",
              "      <td>Hydrogenases were measured in intact actinorhi...</td>\n",
              "      <td>hydrogenas measur intact actinorhiz root nodul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>c07ab857fdbb5641ef0ac1437c2533b0ded2ac55</td>\n",
              "      <td>Global Motion Planning using a Laplacian Poten...</td>\n",
              "      <td>This paper describes a motion planning method ...</td>\n",
              "      <td>paper describ motion plan method use artif pot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>2337681c4cfcfbab22614618212f1f98c17087a1</td>\n",
              "      <td>Emotional intelligence and leadership in organ...</td>\n",
              "      <td>OF THE DISSERTATION EMOTIONAL INTELLGENCE AND ...</td>\n",
              "      <td>dissert emot intellg leadership organ metaanal...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>300 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-943f3a17-46b7-4a09-8aa8-2f095189db7c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-943f3a17-46b7-4a09-8aa8-2f095189db7c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-943f3a17-46b7-4a09-8aa8-2f095189db7c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5359be94-3e2c-4097-b610-6903ad894fac\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5359be94-3e2c-4097-b610-6903ad894fac')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5359be94-3e2c-4097-b610-6903ad894fac button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_5295d0ad-ff15-4273-8bd1-e6febfd8954f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_5295d0ad-ff15-4273-8bd1-e6febfd8954f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 300,\n  \"fields\": [\n    {\n      \"column\": \"paperId\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 300,\n        \"samples\": [\n          \"a96888d9c95afa4a5af87825c11a2e024a8e71fd\",\n          \"e11fdd56395503694deb791496bac43c67741f63\",\n          \"b78970efdd3a2897db9ff0e7638b5212fe84d659\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 299,\n        \"samples\": [\n          \"Organisation of experimental thrombosis by blood cells\",\n          \"Computer Chess Compendium\",\n          \"The effect of nitrogen deficiency, methionine and inhibitors of methionine metabolism on the DMSP contents of Tetraselmis subcordiformis (Stein)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 239,\n        \"samples\": [\n          \"This paper presents a review of the literature on network traffic prediction, while also serving as a tutorial to the topic. We examine works based on autoregressive moving average models, like ARMA, ARIMA and SARIMA, as well as works based on Artifical Neural Networks approaches, such as RNN, LSTM, GRU, and CNN. In all cases, we provide a complete and self-contained presentation of the mathematical foundations of each technique, which allows the reader to get a full understanding of the operation of the different proposed methods. Further, we perform numerical experiments based on real data sets, which allows comparing the various approaches directly in terms of fitting quality and computational costs. We make our code publicly available, so that readers can readily access a wide range of forecasting tools, and possibly use them as benchmarks for more advanced solutions.\",\n          \"Modern, globalized lifestyles make physical separation of family members an inevitable part of life. Due to changes in routines and social roles, remaining connected to distant loved ones is challenging and adversely affects psychological well-being. To support relatedness, we created FamilyFlower, an artificial flower aimed at fostering a connection between remote households by bringing awareness of everyday activities and offering basic expressivity. In its environment, each device detects human presence, movement, sound, and touch. In the remote household, the paired prototype responds by respectively opening the flower bud, actuating the stem, altering the seeds\\u2019 color, and dispensing a fragrance. In a 2-week study of deployment between 2 family members, we collected initial impressions of our prototype, and identified key aspects our system could support in future research. From our results, we see that FamilyFlower supports an awareness of ongoing activities in a distant household and built a feeling of connectedness.\",\n          \"This paper presents a method to pre-train transformer-based encoder-decoder automatic speech recognition (ASR) models using sufficient target-domain text. During pre-training, we train the transformer decoder as a conditional language model with empty or artifical states, rather than the real encoder states. By this pre-training strategy, the decoder can learn how to generate grammatical text sequence before learning how to generate correct transcriptions. Contrast to other methods which utilize text only data to improve the ASR performance, our method does not change the network architecture of the ASR model or introduce extra component like text-to-speech (TTS) or text-to-encoder (TTE). Experimental results on LibriSpeech corpus show that the proposed method can relatively reduce the word error rate over 10%, using 960 hours transcriptions.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 238,\n        \"samples\": [\n          \"samplerel invari import topic psychometr research generaliz find broad rang applic sampl requir equival interpret base measur outcom across respect sampl contextu factor like gender age cultur ethnic socioeconom statu etc may affect mean interpret psycholog measur samplerel invari frequent investig use multiplegroup mean covari structur mgmc analys method build upon natur artif categori contextu variabl mani contextu variabl continu variabl categori zation associ inform loss potenti overli simplist data analys present discus two complementari analyt approach latent moder structur lm equat local structur equat model lsem approach allow treat contextu factor continu variabl appropri detect nonlinear relat use method exemplifi base real data investig measur equival batteri cognit test across age n age rang year base higherord factor model cognit abil factori equival could establish contradict agededifferenti hypothesi advantag disadvantag mgmc lm lsem implement beyond agingresearch discus\",\n          \"simul one five key technolog pwc artifici intellig acceler lab use build artifici intellig ai applic applic ai acceler rapidli spawn new sector result unpreced reach power influenc simul explicitli captur behavior agent process either describ replac ai compon ai compon embed simul provid learn adapt behavior simul use evalu impact introduc ai real world system suppli chain product process workshop demonstr agentbas model reinforc learn autonom fleet coordin demonstr describ detail version anylog consum market model modifi includ adapt dynam base deep learn describ approach integr machin learn design develop simul\",\n          \"yearold artifici pace woman dizzi extrem low physic capac exhibit systol bp vari one moment anoth stand posit measur aid strain gaug techniqu amplitud pul wave left thumb record shown vari wide variat correl synchroni asynchroni atrial ventricular activ pronounc decreas stroke volum peripher pul volum record pacemakerinduc beat compar idioventricular beat artif stimul rate min thu avoid competit still protect syncop free symptom\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (15 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f577015-ba4e-474e-e676-33074d27f582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- POS Tagging Results ----\n",
            "Total Nouns      : 16526\n",
            "Total Verbs      : 1983\n",
            "Total Adjectives : 2131\n",
            "Total Adverbs    : 275\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# 1. Parts of Speech (POS) Tagging:\n",
        "noun_count = 0\n",
        "verb_count = 0\n",
        "adj_count = 0\n",
        "adv_count = 0\n",
        "\n",
        "# Process each cleaned text\n",
        "for text in df['clean_text'].dropna():\n",
        "    doc = nlp(text)\n",
        "    for token in doc:\n",
        "        # Count both common and proper nouns as nouns.\n",
        "        if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
        "            noun_count += 1\n",
        "        elif token.pos_ == \"VERB\":\n",
        "            verb_count += 1\n",
        "        elif token.pos_ == \"ADJ\":\n",
        "            adj_count += 1\n",
        "        elif token.pos_ == \"ADV\":\n",
        "            adv_count += 1\n",
        "\n",
        "print(\"---- POS Tagging Results ----\")\n",
        "print(\"Total Nouns      :\", noun_count)\n",
        "print(\"Total Verbs      :\", verb_count)\n",
        "print(\"Total Adjectives :\", adj_count)\n",
        "print(\"Total Adverbs    :\", adv_count)\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entity_counter = Counter()\n",
        "entity_types = [\"PERSON\", \"ORG\", \"GPE\", \"PRODUCT\", \"DATE\"]\n",
        "\n",
        "for text in df['clean_text'].dropna():\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in entity_types:\n",
        "            entity_counter[ent.label_] += 1\n",
        "\n",
        "print(\"---- Named Entity Recognition Counts ----\")\n",
        "for ent_type in entity_types:\n",
        "    print(f\"{ent_type:8}: {entity_counter[ent_type]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFrtGjaMdQNG",
        "outputId": "0ba49c4f-e17f-426a-cc19-9790314f79f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Named Entity Recognition Counts ----\n",
            "PERSON  : 504\n",
            "ORG     : 338\n",
            "GPE     : 132\n",
            "PRODUCT : 11\n",
            "DATE    : 48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "#I choosed first sentance from the cleaned text\n",
        "example_doc = None\n",
        "for text in df['clean_text'].dropna():\n",
        "    doc = nlp(text)\n",
        "    if list(doc.sents):\n",
        "        example_doc = doc\n",
        "        break\n",
        "\n",
        "if example_doc is None:\n",
        "    print(\"No sentence found in the CSV file for dependency parsing.\")\n",
        "else:\n",
        "    # first sentance\n",
        "    example_sentence = list(example_doc.sents)[0]\n",
        "    print(\"---- Example Sentence for Dependency Parsing ----\")\n",
        "    print(\"Sentence:\")\n",
        "    print(example_sentence.text)\n",
        "    print(\"\\nDependency Parsing Details:\")\n",
        "\n",
        "\n",
        "    for token in example_sentence:\n",
        "        print(f\"{token.text:12} {token.dep_:12} {token.head.text}\")\n",
        "\n",
        "    print(\"\\nExplanation:\")\n",
        "    print(\"Each line shows a token (word) from the sentence, its dependency relation (dep_),\")\n",
        "    print(\"and the head token to which it is connected. This dependency structure reveals\")\n",
        "    print(\"the grammatical relationships within the sentence, such as which word is the subject,\")\n",
        "    print(\"object, modifier, etc., relative to the main verb (ROOT) of the sentence.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3N2vlNl0cwQ7",
        "outputId": "0ac3b437-c567-4bc6-c6c2-afef1a9389eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- Example Sentence for Dependency Parsing ----\n",
            "Sentence:\n",
            "day use ai becam part educ differ method use enhanc know student smart board interact panel differ app like attend onlin class etc along good disadvantag use ai daytoday life educ student lack interest toward ethic educ run onlin solut studi may get easi learn slowli unknow addict mobil tab lack interact teacher peer share knowledg well doubt\n",
            "\n",
            "Dependency Parsing Details:\n",
            "day          compound     use\n",
            "use          nsubj        ai\n",
            "ai           nsubj        differ\n",
            "becam        compound     educ\n",
            "part         compound     educ\n",
            "educ         nsubj        differ\n",
            "differ       ROOT         differ\n",
            "method       compound     use\n",
            "use          compound     enhanc\n",
            "enhanc       nsubj        know\n",
            "know         compound     panel\n",
            "student      compound     interact\n",
            "smart        compound     interact\n",
            "board        compound     interact\n",
            "interact     compound     panel\n",
            "panel        nsubj        differ\n",
            "differ       ccomp        differ\n",
            "app          dobj         differ\n",
            "like         prep         differ\n",
            "attend       pcomp        like\n",
            "onlin        compound     class\n",
            "class        dobj         attend\n",
            "etc          advmod       differ\n",
            "along        prep         differ\n",
            "good         amod         use\n",
            "disadvantag  compound     use\n",
            "use          pobj         along\n",
            "ai           aux          educ\n",
            "daytoday     compound     life\n",
            "life         npadvmod     educ\n",
            "educ         conj         differ\n",
            "student      compound     lack\n",
            "lack         compound     interest\n",
            "interest     dobj         educ\n",
            "toward       prep         interest\n",
            "ethic        amod         educ\n",
            "educ         pobj         toward\n",
            "run          advcl        differ\n",
            "onlin        compound     solut\n",
            "solut        compound     studi\n",
            "studi        nsubj        get\n",
            "may          aux          get\n",
            "get          conj         differ\n",
            "easi         amod         knowledg\n",
            "learn        amod         slowli\n",
            "slowli       amod         knowledg\n",
            "unknow       amod         addict\n",
            "addict       compound     mobil\n",
            "mobil        compound     knowledg\n",
            "tab          compound     lack\n",
            "lack         compound     interact\n",
            "interact     compound     knowledg\n",
            "teacher      compound     knowledg\n",
            "peer         compound     share\n",
            "share        compound     knowledg\n",
            "knowledg     dobj         get\n",
            "well         advmod       doubt\n",
            "doubt        conj         differ\n",
            "\n",
            "Explanation:\n",
            "Each line shows a token (word) from the sentence, its dependency relation (dep_),\n",
            "and the head token to which it is connected. This dependency structure reveals\n",
            "the grammatical relationships within the sentence, such as which word is the subject,\n",
            "object, modifier, etc., relative to the main verb (ROOT) of the sentence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Following Questions must answer using AI assitance**"
      ],
      "metadata": {
        "id": "EcVqy1yj3wja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4 (20 points)."
      ],
      "metadata": {
        "id": "kEdcyHX8VaDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. (PART-1)\n",
        "Web scraping data from the GitHub Marketplace to gather details about popular actions. Using Python, the process begins by sending HTTP requests to multiple pages of the marketplace (1000 products), handling pagination through dynamic page numbers. The key details extracted include the product name, a short description, and the URL.\n",
        "\n",
        " The extracted data is stored in a structured CSV format with columns for product name, description, URL, and page number. A time delay is introduced between requests to avoid server overload. ChatGPT can assist by helping with the parsing of HTML, error handling, and generating reports based on the data collected.\n",
        "\n",
        " The goal is to complete the scraping within a specified time limit, ensuring that the process is efficient and adheres to GitHub’s usage guidelines.\n",
        "\n",
        "(PART -2)\n",
        "\n",
        "1.   **Preprocess Data**: Clean the text by tokenizing, removing stopwords, and converting to lowercase.\n",
        "\n",
        "2. Perform **Data Quality** operations.\n",
        "\n",
        "\n",
        "Preprocessing:\n",
        "Preprocessing involves cleaning the text by removing noise such as special characters, HTML tags, and unnecessary whitespace. It also includes tasks like tokenization, stopword removal, and lemmatization to standardize the text for analysis.\n",
        "\n",
        "Data Quality:\n",
        "Data quality checks ensure completeness, consistency, and accuracy by verifying that all required columns are filled and formatted correctly. Additionally, it involves identifying and removing duplicates, handling missing values, and ensuring the data reflects the true content accurately.\n"
      ],
      "metadata": {
        "id": "1Ung5_YW3C6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Github MarketPlace page:\n",
        "https://github.com/marketplace?type=actions"
      ],
      "metadata": {
        "id": "CTOfUpatronW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROMPT: Act like a senior web scraper and webscrape this page.\n",
        "\n",
        "https://github.com/marketplace?type=actions\n",
        "\n",
        "there are multipel pages i need to scrape around 1000 products. so, make sure follow pagination and keep a 3 sec timmer\n",
        "here is the pagination link\n",
        "https://github.com/marketplace?page=2&type=actions"
      ],
      "metadata": {
        "id": "I6z9DWyyecge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#response:\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "# Base URL for GitHub Marketplace (Actions)\n",
        "BASE_URL = \"https://github.com/marketplace\"\n",
        "# Query parameters: type is \"actions\"\n",
        "params = {\n",
        "    \"type\": \"actions\",\n",
        "    \"page\": 1\n",
        "}\n",
        "\n",
        "products = []            # To store scraped product data\n",
        "max_products = 1000      # Target number of products\n",
        "current_page = 1         # Starting page\n",
        "\n",
        "while len(products) < max_products:\n",
        "    print(f\"Scraping page {current_page}...\")\n",
        "\n",
        "    # Update the page number in the query parameters\n",
        "    params['page'] = current_page\n",
        "    response = requests.get(BASE_URL)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error fetching page {current_page} (status code {response.status_code}).\")\n",
        "        break\n",
        "\n",
        "    # Parse the HTML using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find product cards. Inspect the page to determine the correct selector.\n",
        "    # The following selector ('.application-card') is a placeholder and may need to be adjusted.\n",
        "    product_cards = soup.select('.application-card')\n",
        "\n",
        "    # If no products are found, we assume we've reached the end of pagination.\n",
        "    if not product_cards:\n",
        "        print(\"No more product cards found. Ending pagination.\")\n",
        "        break\n",
        "\n",
        "    # Loop through each product card and extract details\n",
        "    for card in product_cards:\n",
        "        # Example extraction: adjust selectors based on actual HTML structure.\n",
        "        title_tag = card.find('h3')\n",
        "        title = title_tag.text.strip() if title_tag else \"No title\"\n",
        "\n",
        "        link_tag = card.find('a', href=True)\n",
        "        link = link_tag['href'] if link_tag else \"No link\"\n",
        "\n",
        "        # Append the product information\n",
        "        products.append({\n",
        "            \"title\": title,\n",
        "            \"link\": link\n",
        "        })\n",
        "\n",
        "    print(f\"Found {len(product_cards)} products on page {current_page}. Total so far: {len(products)}\")\n",
        "\n",
        "    # If we've reached or exceeded the target, break out of the loop.\n",
        "    if len(products) >= max_products:\n",
        "        print(\"Reached target product count.\")\n",
        "        break\n",
        "\n",
        "    # Prepare to scrape the next page\n",
        "    current_page += 1\n",
        "    # Respect a 3-second delay between requests\n",
        "    time.sleep(3)\n",
        "\n",
        "print(f\"Scraped {len(products)} products in total.\")\n",
        "\n",
        "# Optionally, print out the first few products\n",
        "for idx, product in enumerate(products[:10], start=1):\n",
        "    print(f\"{idx}. {product}\")\n"
      ],
      "metadata": {
        "id": "4dtco9K--ks6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "797fc109-6284-4469-8a7c-2005be2cd7fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Error fetching page 1 (status code 400).\n",
            "Scraped 0 products in total.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROMPT: I have given a div element as input and asked to extract informaion."
      ],
      "metadata": {
        "id": "mIYkD2kHfZF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Response:\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "BASE_URL = \"https://github.com/marketplace\"\n",
        "params = {\n",
        "    \"type\": \"actions\",\n",
        "    \"page\": 1\n",
        "}\n",
        "\n",
        "products = []       # List to store scraped products\n",
        "max_products = 1000 # Target product count\n",
        "current_page = 1    # Start on page 1\n",
        "\n",
        "while len(products) < max_products:\n",
        "    print(f\"Scraping page {current_page}...\")\n",
        "\n",
        "    # Update the page parameter for pagination\n",
        "    params['page'] = current_page\n",
        "    response = requests.get(BASE_URL, params=params)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error fetching page {current_page} (status code {response.status_code}).\")\n",
        "        break\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Locate each product element using the data-testid attribute.\n",
        "    product_elements = soup.find_all(\"div\", {\"data-testid\": \"non-featured-item\"})\n",
        "\n",
        "    if not product_elements:\n",
        "        print(\"No more product items found. Ending pagination.\")\n",
        "        break\n",
        "\n",
        "    for element in product_elements:\n",
        "        # Extract the title from the <a> inside the <h3>\n",
        "        title_element = element.select_one(\"h3 a\")\n",
        "        title = title_element.get_text(strip=True) if title_element else \"No title\"\n",
        "\n",
        "        # Extract the link from the <a> tag; if it's relative, prepend the domain.\n",
        "        link = title_element[\"href\"] if title_element and \"href\" in title_element.attrs else \"No link\"\n",
        "        if link.startswith(\"/\"):\n",
        "            link = \"https://github.com\" + link\n",
        "\n",
        "        # Extract the description from the <p> element.\n",
        "        description_element = element.find(\"p\")\n",
        "        description = description_element.get_text(strip=True) if description_element else \"No description\"\n",
        "\n",
        "        # Optionally, extract the listing type from the <span> element.\n",
        "        label_element = element.find(\"span\", {\"data-testid\": \"listing-type-label\"})\n",
        "        label = label_element.get_text(strip=True) if label_element else \"No label\"\n",
        "\n",
        "        # Append the product info to the list.\n",
        "        products.append({\n",
        "            \"title\": title,\n",
        "            \"link\": link,\n",
        "            \"description\": description,\n",
        "            \"label\": label\n",
        "        })\n",
        "\n",
        "    print(f\"Found {len(product_elements)} products on page {current_page}. Total so far: {len(products)}\")\n",
        "\n",
        "    if len(products) >= max_products:\n",
        "        print(\"Reached target product count.\")\n",
        "        break\n",
        "\n",
        "    current_page += 1\n",
        "    time.sleep(3)  # Respect a 3-second delay between requests\n",
        "\n",
        "print(f\"Scraped {len(products)} products in total.\")\n",
        "\n",
        "# Optionally, display the first few products.\n",
        "for idx, product in enumerate(products[:10], start=1):\n",
        "    print(f\"{idx}. {product}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dq0YIOfpfil4",
        "outputId": "1f5d9b46-d67d-4e7a-e6cd-2a4f254dfedf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Error fetching page 1 (status code 400).\n",
            "Scraped 0 products in total.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROMPT:The extracted data is stored in a structured CSV format with columns for product name, description, URL, and page number. A time delay is introduced between requests to avoid server overload. ChatGPT can assist by helping with the parsing of HTML, error handling, and generating reports based on the data collected."
      ],
      "metadata": {
        "id": "3rQWjIUQgEYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import csv\n",
        "\n",
        "# Base URL and query parameters for GitHub Marketplace (Actions)\n",
        "BASE_URL = \"https://github.com/marketplace?page=1&type=actions\"\n",
        "params = {\n",
        "    \"type\": \"actions\",\n",
        "    \"page\": 1  # will be updated with pagination\n",
        "}\n",
        "\n",
        "output_file = \"github_marketplace.csv\"  # CSV output file\n",
        "max_products = 1000  # target number of products\n",
        "current_page = 1    # starting page\n",
        "products = []       # list to hold product data\n",
        "\n",
        "while len(products) < max_products:\n",
        "    print(f\"Scraping page {current_page}...\")\n",
        "    params['page'] = current_page\n",
        "    try:\n",
        "        response = requests.get(BASE_URL, params=params, timeout=10)\n",
        "        response.raise_for_status()  # raises an HTTPError for bad responses\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Request failed for page {current_page}: {e}\")\n",
        "        break\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Locate each product element using the data-testid attribute.\n",
        "    product_elements = soup.find_all(\"div\", {\"data-testid\": \"non-featured-item\"})\n",
        "\n",
        "    if not product_elements:\n",
        "        print(\"No product items found. Ending pagination.\")\n",
        "        break\n",
        "\n",
        "    for element in product_elements:\n",
        "        try:\n",
        "            # Extract the product name (title) from the <a> tag inside the <h3>\n",
        "            title_element = element.select_one(\"h3 a\")\n",
        "            title = title_element.get_text(strip=True) if title_element else \"No title\"\n",
        "\n",
        "            # Extract the URL and adjust if it's a relative link\n",
        "            link = title_element[\"href\"] if title_element and \"href\" in title_element.attrs else \"No link\"\n",
        "            if link.startswith(\"/\"):\n",
        "                link = \"https://github.com\" + link\n",
        "\n",
        "            # Extract the description from the <p> element\n",
        "            description_element = element.find(\"p\")\n",
        "            description = description_element.get_text(strip=True) if description_element else \"No description\"\n",
        "\n",
        "            # Append product details along with the page number\n",
        "            products.append({\n",
        "                \"product name\": title,\n",
        "                \"description\": description,\n",
        "                \"URL\": link,\n",
        "                \"page number\": current_page\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing product element on page {current_page}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Found {len(product_elements)} products on page {current_page}. Total so far: {len(products)}\")\n",
        "\n",
        "    if len(products) >= max_products:\n",
        "        print(\"Reached target product count.\")\n",
        "        break\n",
        "\n",
        "    current_page += 1\n",
        "    time.sleep(3)  # Wait 3 seconds before next request\n",
        "\n",
        "# Write the collected product data to a CSV file\n",
        "csv_columns = [\"product name\", \"description\", \"URL\", \"page number\"]\n",
        "try:\n",
        "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
        "        writer.writeheader()\n",
        "        for product in products:\n",
        "            writer.writerow(product)\n",
        "    print(f\"Data successfully written to {output_file}.\")\n",
        "except IOError as e:\n",
        "    print(f\"I/O error while writing CSV: {e}\")\n",
        "\n",
        "# Generate a simple report\n",
        "print(\"\\nScraping Report:\")\n",
        "print(f\"Total products scraped: {len(products)}\")\n",
        "pages_scraped = current_page - 1\n",
        "print(f\"Total pages scraped: {pages_scraped}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIW15ZZqgF6N",
        "outputId": "e5b88ac3-1f60-4638-eb20-6c231c83200b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Request failed for page 1: 400 Client Error: Bad Request for url: https://github.com/marketplace?type=actions&page=1\n",
            "Data successfully written to github_marketplace.csv.\n",
            "\n",
            "Scraping Report:\n",
            "Total products scraped: 0\n",
            "Total pages scraped: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROMPT =\"https://github.com/marketplace?page=1&type=actions\"\n",
        "\n",
        "here is the url, create a for loop , for i change the page={i} untill max_products <1000"
      ],
      "metadata": {
        "id": "u_IIkT2jglWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import csv\n",
        "\n",
        "# Output CSV file name\n",
        "output_file = \"github_marketplace.csv\"\n",
        "\n",
        "# Set target number of products to scrape\n",
        "max_products = 1000\n",
        "products = []  # List to hold all scraped product data\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:123.0) Gecko/20100101 Firefox/123.0\",\n",
        "    \"Accept\": \"text/html\",\n",
        "    \"Connection\": \"keep-alive\"\n",
        "}\n",
        "\n",
        "# Use the provided link as a template. We'll update the page number dynamically.\n",
        "base_url_template = \"https://github.com/marketplace?page={}&type=actions\"\n",
        "\n",
        "# We'll iterate over a range of page numbers starting at 2.\n",
        "# Adjust the range if needed. This loop will break if no products are found or 1000+ products are scraped.\n",
        "for i in range(1, 100):\n",
        "    if len(products) >= max_products:\n",
        "        break\n",
        "\n",
        "    # Build the URL for the current page\n",
        "    url = f\"https://github.com/marketplace?page={i}&type=actions\"\n",
        "    print(f\"Scraping page {i}\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()  # Raise exception for HTTP errors\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching page {i}: {e}\")\n",
        "        break\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all product elements using the provided attribute.\n",
        "    # This selector is based on the snippet provided.\n",
        "    product_elements = soup.find_all(\"div\", {\"data-testid\": \"non-featured-item\"})\n",
        "\n",
        "    for element in product_elements:\n",
        "        try:\n",
        "            # Extract the product name from the <a> tag inside the <h3> element.\n",
        "            title_element = element.select_one(\"h3 a\")\n",
        "            product_name = title_element.get_text(strip=True) if title_element else \"No title\"\n",
        "\n",
        "            # Extract the URL and adjust if it's relative.\n",
        "            url_value = title_element[\"href\"] if title_element and \"href\" in title_element.attrs else \"No link\"\n",
        "            if url_value.startswith(\"/\"):\n",
        "                url_value = \"https://github.com\" + url_value\n",
        "\n",
        "            # Extract the product description from the <p> tag.\n",
        "            description_element = element.find(\"p\")\n",
        "            description = description_element.get_text(strip=True) if description_element else \"No description\"\n",
        "\n",
        "            # Save the product data including the current page number.\n",
        "            products.append({\n",
        "                \"product name\": product_name,\n",
        "                \"description\": description,\n",
        "                \"URL\": url_value,\n",
        "                \"page number\": i\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing a product on page {i}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Found {len(product_elements)} products on page {i}. Total products so far: {len(products)}\")\n",
        "\n",
        "    # Respect a 3-second delay between page requests\n",
        "    time.sleep(3)\n",
        "\n",
        "# Write the collected product data to a CSV file.\n",
        "csv_columns = [\"product name\", \"description\", \"URL\", \"page number\"]\n",
        "try:\n",
        "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(products)\n",
        "    print(f\"Data successfully written to {output_file}.\")\n",
        "except IOError as e:\n",
        "    print(f\"I/O error while writing CSV: {e}\")\n",
        "\n",
        "# Print a brief report.\n",
        "print(\"\\nScraping Report:\")\n",
        "print(f\"Total products scraped: {len(products)}\")\n",
        "print(f\"Pages processed: {i - 1}\")  # Since i is incremented even if loop breaks\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CvdbHJ5gnYd",
        "outputId": "9801bf02-7804-4d97-8b7f-0fb322eb9d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1\n",
            "Found 20 products on page 1. Total products so far: 20\n",
            "Scraping page 2\n",
            "Found 20 products on page 2. Total products so far: 40\n",
            "Scraping page 3\n",
            "Found 20 products on page 3. Total products so far: 60\n",
            "Scraping page 4\n",
            "Found 20 products on page 4. Total products so far: 80\n",
            "Scraping page 5\n",
            "Found 20 products on page 5. Total products so far: 100\n",
            "Scraping page 6\n",
            "Found 20 products on page 6. Total products so far: 120\n",
            "Scraping page 7\n",
            "Found 20 products on page 7. Total products so far: 140\n",
            "Scraping page 8\n",
            "Found 0 products on page 8. Total products so far: 140\n",
            "Scraping page 9\n",
            "Found 20 products on page 9. Total products so far: 160\n",
            "Scraping page 10\n",
            "Found 0 products on page 10. Total products so far: 160\n",
            "Scraping page 11\n",
            "Found 20 products on page 11. Total products so far: 180\n",
            "Scraping page 12\n",
            "Found 20 products on page 12. Total products so far: 200\n",
            "Scraping page 13\n",
            "Found 20 products on page 13. Total products so far: 220\n",
            "Scraping page 14\n",
            "Found 20 products on page 14. Total products so far: 240\n",
            "Scraping page 15\n",
            "Found 20 products on page 15. Total products so far: 260\n",
            "Scraping page 16\n",
            "Found 0 products on page 16. Total products so far: 260\n",
            "Scraping page 17\n",
            "Found 20 products on page 17. Total products so far: 280\n",
            "Scraping page 18\n",
            "Found 20 products on page 18. Total products so far: 300\n",
            "Scraping page 19\n",
            "Found 20 products on page 19. Total products so far: 320\n",
            "Scraping page 20\n",
            "Found 20 products on page 20. Total products so far: 340\n",
            "Scraping page 21\n",
            "Found 20 products on page 21. Total products so far: 360\n",
            "Scraping page 22\n",
            "Found 20 products on page 22. Total products so far: 380\n",
            "Scraping page 23\n",
            "Found 20 products on page 23. Total products so far: 400\n",
            "Scraping page 24\n",
            "Found 20 products on page 24. Total products so far: 420\n",
            "Scraping page 25\n",
            "Found 20 products on page 25. Total products so far: 440\n",
            "Scraping page 26\n",
            "Found 20 products on page 26. Total products so far: 460\n",
            "Scraping page 27\n",
            "Found 20 products on page 27. Total products so far: 480\n",
            "Scraping page 28\n",
            "Found 20 products on page 28. Total products so far: 500\n",
            "Scraping page 29\n",
            "Found 20 products on page 29. Total products so far: 520\n",
            "Scraping page 30\n",
            "Found 20 products on page 30. Total products so far: 540\n",
            "Scraping page 31\n",
            "Found 20 products on page 31. Total products so far: 560\n",
            "Scraping page 32\n",
            "Found 20 products on page 32. Total products so far: 580\n",
            "Scraping page 33\n",
            "Found 20 products on page 33. Total products so far: 600\n",
            "Scraping page 34\n",
            "Found 20 products on page 34. Total products so far: 620\n",
            "Scraping page 35\n",
            "Found 20 products on page 35. Total products so far: 640\n",
            "Scraping page 36\n",
            "Found 20 products on page 36. Total products so far: 660\n",
            "Scraping page 37\n",
            "Found 20 products on page 37. Total products so far: 680\n",
            "Scraping page 38\n",
            "Found 20 products on page 38. Total products so far: 700\n",
            "Scraping page 39\n",
            "Found 20 products on page 39. Total products so far: 720\n",
            "Scraping page 40\n",
            "Found 20 products on page 40. Total products so far: 740\n",
            "Scraping page 41\n",
            "Found 20 products on page 41. Total products so far: 760\n",
            "Scraping page 42\n",
            "Found 20 products on page 42. Total products so far: 780\n",
            "Scraping page 43\n",
            "Found 20 products on page 43. Total products so far: 800\n",
            "Scraping page 44\n",
            "Found 20 products on page 44. Total products so far: 820\n",
            "Scraping page 45\n",
            "Found 20 products on page 45. Total products so far: 840\n",
            "Scraping page 46\n",
            "Found 20 products on page 46. Total products so far: 860\n",
            "Scraping page 47\n",
            "Found 20 products on page 47. Total products so far: 880\n",
            "Scraping page 48\n",
            "Found 20 products on page 48. Total products so far: 900\n",
            "Scraping page 49\n",
            "Found 20 products on page 49. Total products so far: 920\n",
            "Scraping page 50\n",
            "Found 20 products on page 50. Total products so far: 940\n",
            "Scraping page 51\n",
            "Found 20 products on page 51. Total products so far: 960\n",
            "Scraping page 52\n",
            "Found 20 products on page 52. Total products so far: 980\n",
            "Scraping page 53\n",
            "Found 20 products on page 53. Total products so far: 1000\n",
            "Data successfully written to github_marketplace.csv.\n",
            "\n",
            "Scraping Report:\n",
            "Total products scraped: 1000\n",
            "Pages processed: 53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "file_path_github = '/content/github_marketplace.csv'\n",
        "github_df = pd.read_csv(file_path_github)\n",
        "github_df.head()"
      ],
      "metadata": {
        "id": "Ik17sR-T6-0P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "8a121552-d399-4689-aee9-54db52e32550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   product name  \\\n",
              "0                TruffleHog OSS   \n",
              "1                 Metrics embed   \n",
              "2  yq - portable yaml processor   \n",
              "3                  Super-Linter   \n",
              "4        Gosec Security Checker   \n",
              "\n",
              "                                         description  \\\n",
              "0                Scan Github Actions with TruffleHog   \n",
              "1  An infographics generator with 40+ plugins and...   \n",
              "2  create, read, update, delete, merge, validate ...   \n",
              "3  Super-linter is a ready-to-run collection of l...   \n",
              "4                    Runs the gosec security checker   \n",
              "\n",
              "                                                 URL  page number  \n",
              "0  https://github.com/marketplace/actions/truffle...            1  \n",
              "1  https://github.com/marketplace/actions/metrics...            1  \n",
              "2  https://github.com/marketplace/actions/yq-port...            1  \n",
              "3  https://github.com/marketplace/actions/super-l...            1  \n",
              "4  https://github.com/marketplace/actions/gosec-s...            1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7d1e746a-53cc-4c02-b1fe-64487a792820\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>product name</th>\n",
              "      <th>description</th>\n",
              "      <th>URL</th>\n",
              "      <th>page number</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TruffleHog OSS</td>\n",
              "      <td>Scan Github Actions with TruffleHog</td>\n",
              "      <td>https://github.com/marketplace/actions/truffle...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Metrics embed</td>\n",
              "      <td>An infographics generator with 40+ plugins and...</td>\n",
              "      <td>https://github.com/marketplace/actions/metrics...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>yq - portable yaml processor</td>\n",
              "      <td>create, read, update, delete, merge, validate ...</td>\n",
              "      <td>https://github.com/marketplace/actions/yq-port...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Super-Linter</td>\n",
              "      <td>Super-linter is a ready-to-run collection of l...</td>\n",
              "      <td>https://github.com/marketplace/actions/super-l...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Gosec Security Checker</td>\n",
              "      <td>Runs the gosec security checker</td>\n",
              "      <td>https://github.com/marketplace/actions/gosec-s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7d1e746a-53cc-4c02-b1fe-64487a792820')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7d1e746a-53cc-4c02-b1fe-64487a792820 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7d1e746a-53cc-4c02-b1fe-64487a792820');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5b9b561c-7f11-4a89-9f31-23a90567e6aa\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5b9b561c-7f11-4a89-9f31-23a90567e6aa')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5b9b561c-7f11-4a89-9f31-23a90567e6aa button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "github_df",
              "summary": "{\n  \"name\": \"github_df\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"product name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 995,\n        \"samples\": [\n          \"Azure Pipelines Action\",\n          \"My GitHub Status\",\n          \"UsePSDevOps\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 988,\n        \"samples\": [\n          \"Automating code review using ChatGPT with Github Actions integration on PRs for code explanations\",\n          \"Push a subdirectory as a branch to any git repo over SSH (or to the local repo)\",\n          \"An action to create an issue using content from a file\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"URL\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 995,\n        \"samples\": [\n          \"https://github.com/marketplace/actions/azure-pipelines-action\",\n          \"https://github.com/marketplace/actions/my-github-status\",\n          \"https://github.com/marketplace/actions/usepsdevops\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 15,\n        \"min\": 1,\n        \"max\": 53,\n        \"num_unique_values\": 50,\n        \"samples\": [\n          17,\n          43,\n          34\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt:ok , I got the ouptu, now i want to perform these operations\n",
        "the file is in dataframe named as github_df\n",
        "\n"
      ],
      "metadata": {
        "id": "IfV1VUUo3Zop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#response:\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download required NLTK resources if not already available\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab ')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Define a text cleaning function\n",
        "def clean_text(text):\n",
        "    if pd.isnull(text):\n",
        "        return \"\"\n",
        "    # Remove HTML tags (if any)\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters and digits, leaving only letters and whitespace\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Lemmatize tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    # Return the cleaned tokens as a joined string (or you can return the tokens list)\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Example DataFrame: Assuming github_df is already loaded with columns:\n",
        "# \"product name\", \"description\", \"URL\", and \"page number\"\n",
        "# For demonstration, we might preview the DataFrame:\n",
        "print(\"Before preprocessing:\")\n",
        "print(github_df.head())\n",
        "\n",
        "# Preprocess the text fields:\n",
        "# Clean the \"product name\" column\n",
        "github_df['clean_product_name'] = github_df['product name'].apply(clean_text)\n",
        "# Clean the \"description\" column\n",
        "github_df['clean_description'] = github_df['description'].apply(clean_text)\n",
        "\n",
        "# ------------------------------\n",
        "# Data Quality Checks & Operations\n",
        "# ------------------------------\n",
        "\n",
        "# 1. Check for missing values in required columns\n",
        "required_columns = [\"product name\", \"description\", \"URL\"]\n",
        "missing_values = github_df[required_columns].isnull().sum()\n",
        "print(\"\\nMissing values per required column:\")\n",
        "print(missing_values)\n",
        "\n",
        "# 2. Remove rows with missing values in the required columns\n",
        "github_df.dropna(subset=required_columns, inplace=True)\n",
        "\n",
        "# 3. Remove duplicate records based on key columns (e.g., URL or product name)\n",
        "github_df.drop_duplicates(subset=[\"URL\"], inplace=True)\n",
        "\n",
        "# 4. Validate formatting (for example, ensuring that URL starts with 'https://github.com')\n",
        "github_df = github_df[github_df['URL'].str.startswith(\"https://github.com\")]\n",
        "\n",
        "# 5. Additional cleaning: If any remaining whitespace issues exist, you can clean those as well.\n",
        "github_df['product name'] = github_df['product name'].str.strip()\n",
        "github_df['description'] = github_df['description'].str.strip()\n",
        "\n",
        "# Preview the cleaned and quality-checked DataFrame\n",
        "print(\"\\nAfter preprocessing and quality checks:\")\n",
        "print(github_df.head())\n",
        "\n",
        "# Optionally, you can save the cleaned DataFrame back to a CSV file:\n",
        "github_df.to_csv(\"github_marketplace_cleaned.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fl8P4XPj3FOl",
        "outputId": "12a43a81-d2a6-49a0-d93f-56b2e5398742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before preprocessing:\n",
            "                   product name  \\\n",
            "0                TruffleHog OSS   \n",
            "1                 Metrics embed   \n",
            "2  yq - portable yaml processor   \n",
            "3                  Super-Linter   \n",
            "4        Gosec Security Checker   \n",
            "\n",
            "                                         description  \\\n",
            "0                Scan Github Actions with TruffleHog   \n",
            "1  An infographics generator with 40+ plugins and...   \n",
            "2  create, read, update, delete, merge, validate ...   \n",
            "3  Super-linter is a ready-to-run collection of l...   \n",
            "4                    Runs the gosec security checker   \n",
            "\n",
            "                                                 URL  page number  \n",
            "0  https://github.com/marketplace/actions/truffle...            1  \n",
            "1  https://github.com/marketplace/actions/metrics...            1  \n",
            "2  https://github.com/marketplace/actions/yq-port...            1  \n",
            "3  https://github.com/marketplace/actions/super-l...            1  \n",
            "4  https://github.com/marketplace/actions/gosec-s...            1  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Error loading punkt_tab : Package 'punkt_tab ' not found\n",
            "[nltk_data]     in index\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-6beb7f9f311d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Preprocess the text fields:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Clean the \"product name\" column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mgithub_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_product_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgithub_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'product name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;31m# Clean the \"description\" column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mgithub_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_description'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgithub_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-6beb7f9f311d>\u001b[0m in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\s+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Tokenize text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Remove stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yguW6kR4duC",
        "outputId": "88386be5-0161-4249-8d97-19130e93f600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysExa-bw4rJ1",
        "outputId": "f038fe6f-06fe-4eca-fe0a-701c20a39058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download required NLTK resources (run these in a cell or at the start of your script)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Define a safe tokenization function\n",
        "def safe_word_tokenize(text):\n",
        "    try:\n",
        "        tokens = word_tokenize(text)\n",
        "    except LookupError as e:\n",
        "        # If a LookupError occurs, try to download 'punkt'\n",
        "        print(\"LookupError encountered during tokenization. Downloading 'punkt' now...\")\n",
        "        nltk.download('punkt_tab')\n",
        "        tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "def clean_text(text):\n",
        "    if pd.isnull(text):\n",
        "        return \"\"\n",
        "    # Remove HTML tags\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters and digits (keep only letters and whitespace)\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Tokenize text safely\n",
        "    tokens = safe_word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Lemmatize tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    # Return tokens as a single string (or you can return the list of tokens)\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Assuming your DataFrame is loaded as github_df with columns:\n",
        "# \"product name\", \"description\", \"URL\", and \"page number\"\n",
        "\n",
        "print(\"Before preprocessing:\")\n",
        "print(github_df.head())\n",
        "\n",
        "# Preprocess the text fields:\n",
        "github_df['clean_product_name'] = github_df['product name'].apply(clean_text)\n",
        "github_df['clean_description'] = github_df['description'].apply(clean_text)\n",
        "\n",
        "# ------------------------------\n",
        "# Data Quality Checks\n",
        "# ------------------------------\n",
        "\n",
        "# 1. Check for missing values in required columns\n",
        "required_columns = [\"product name\", \"description\", \"URL\"]\n",
        "missing_values = github_df[required_columns].isnull().sum()\n",
        "print(\"\\nMissing values per required column:\")\n",
        "print(missing_values)\n",
        "\n",
        "# 2. Remove rows with missing values in the required columns\n",
        "github_df.dropna(subset=required_columns, inplace=True)\n",
        "\n",
        "# 3. Remove duplicate records based on a key column (e.g., URL)\n",
        "github_df.drop_duplicates(subset=[\"URL\"], inplace=True)\n",
        "\n",
        "# 4. Validate formatting (e.g., ensure URLs start with 'https://github.com')\n",
        "github_df = github_df[github_df['URL'].str.startswith(\"https://github.com\")]\n",
        "\n",
        "# 5. Trim any extra whitespace in text columns\n",
        "github_df['product name'] = github_df['product name'].str.strip()\n",
        "github_df['description'] = github_df['description'].str.strip()\n",
        "\n",
        "print(\"\\nAfter preprocessing and quality checks:\")\n",
        "print(github_df.head())\n",
        "\n",
        "# Optionally, save the cleaned DataFrame to a CSV file:\n",
        "github_df.to_csv(\"github_marketplace_cleaned.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khNLqPGY4jRG",
        "outputId": "c11351df-0508-4612-a3b7-b2344337c126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before preprocessing:\n",
            "                   product name  \\\n",
            "0                TruffleHog OSS   \n",
            "1                 Metrics embed   \n",
            "2  yq - portable yaml processor   \n",
            "3                  Super-Linter   \n",
            "4        Gosec Security Checker   \n",
            "\n",
            "                                         description  \\\n",
            "0                Scan Github Actions with TruffleHog   \n",
            "1  An infographics generator with 40+ plugins and...   \n",
            "2  create, read, update, delete, merge, validate ...   \n",
            "3  Super-linter is a ready-to-run collection of l...   \n",
            "4                    Runs the gosec security checker   \n",
            "\n",
            "                                                 URL  page number  \n",
            "0  https://github.com/marketplace/actions/truffle...            1  \n",
            "1  https://github.com/marketplace/actions/metrics...            1  \n",
            "2  https://github.com/marketplace/actions/yq-port...            1  \n",
            "3  https://github.com/marketplace/actions/super-l...            1  \n",
            "4  https://github.com/marketplace/actions/gosec-s...            1  \n",
            "LookupError encountered during tokenization. Downloading 'punkt' now...\n",
            "\n",
            "Missing values per required column:\n",
            "product name    0\n",
            "description     1\n",
            "URL             0\n",
            "dtype: int64\n",
            "\n",
            "After preprocessing and quality checks:\n",
            "                   product name  \\\n",
            "0                TruffleHog OSS   \n",
            "1                 Metrics embed   \n",
            "2  yq - portable yaml processor   \n",
            "3                  Super-Linter   \n",
            "4        Gosec Security Checker   \n",
            "\n",
            "                                         description  \\\n",
            "0                Scan Github Actions with TruffleHog   \n",
            "1  An infographics generator with 40+ plugins and...   \n",
            "2  create, read, update, delete, merge, validate ...   \n",
            "3  Super-linter is a ready-to-run collection of l...   \n",
            "4                    Runs the gosec security checker   \n",
            "\n",
            "                                                 URL  page number  \\\n",
            "0  https://github.com/marketplace/actions/truffle...            1   \n",
            "1  https://github.com/marketplace/actions/metrics...            1   \n",
            "2  https://github.com/marketplace/actions/yq-port...            1   \n",
            "3  https://github.com/marketplace/actions/super-l...            1   \n",
            "4  https://github.com/marketplace/actions/gosec-s...            1   \n",
            "\n",
            "           clean_product_name  \\\n",
            "0               trufflehog os   \n",
            "1                metric embed   \n",
            "2  yq portable yaml processor   \n",
            "3                super linter   \n",
            "4      gosec security checker   \n",
            "\n",
            "                                   clean_description  \n",
            "0                      scan github action trufflehog  \n",
            "1  infographics generator plugins option display ...  \n",
            "2      create read update delete merge validate yaml  \n",
            "3  super linter ready run collection linters code...  \n",
            "4                         run gosec security checker  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-71517679329b>:29: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a filename than HTML or XML.\n",
            "\n",
            "If you meant to use Beautiful Soup to parse the contents of a file on disk, then something has gone wrong. You should open the file first, using code like this:\n",
            "\n",
            "    filehandle = open(your filename)\n",
            "\n",
            "You can then feed the open filehandle into Beautiful Soup instead of using the filename.\n",
            "\n",
            "However, if you want to parse some data that happens to look like a filename, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
            "\n",
            "    from bs4 import MarkupResemblesLocatorWarning\n",
            "    import warnings\n",
            "\n",
            "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
            "    \n",
            "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "updated_github_file = '/content/github_marketplace_cleaned.csv'\n",
        "updated_github_df = pd.read_csv(updated_github_file)\n",
        "updated_github_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "gZwYNSJ547IU",
        "outputId": "be0d94a4-4676-4545-c0fc-f18025d6c272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   product name  \\\n",
              "0                TruffleHog OSS   \n",
              "1                 Metrics embed   \n",
              "2  yq - portable yaml processor   \n",
              "3                  Super-Linter   \n",
              "4        Gosec Security Checker   \n",
              "\n",
              "                                         description  \\\n",
              "0                Scan Github Actions with TruffleHog   \n",
              "1  An infographics generator with 40+ plugins and...   \n",
              "2  create, read, update, delete, merge, validate ...   \n",
              "3  Super-linter is a ready-to-run collection of l...   \n",
              "4                    Runs the gosec security checker   \n",
              "\n",
              "                                                 URL  page number  \\\n",
              "0  https://github.com/marketplace/actions/truffle...            1   \n",
              "1  https://github.com/marketplace/actions/metrics...            1   \n",
              "2  https://github.com/marketplace/actions/yq-port...            1   \n",
              "3  https://github.com/marketplace/actions/super-l...            1   \n",
              "4  https://github.com/marketplace/actions/gosec-s...            1   \n",
              "\n",
              "           clean_product_name  \\\n",
              "0               trufflehog os   \n",
              "1                metric embed   \n",
              "2  yq portable yaml processor   \n",
              "3                super linter   \n",
              "4      gosec security checker   \n",
              "\n",
              "                                   clean_description  \n",
              "0                      scan github action trufflehog  \n",
              "1  infographics generator plugins option display ...  \n",
              "2      create read update delete merge validate yaml  \n",
              "3  super linter ready run collection linters code...  \n",
              "4                         run gosec security checker  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c0220cf9-f77b-413f-9d16-23022af48fa0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>product name</th>\n",
              "      <th>description</th>\n",
              "      <th>URL</th>\n",
              "      <th>page number</th>\n",
              "      <th>clean_product_name</th>\n",
              "      <th>clean_description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TruffleHog OSS</td>\n",
              "      <td>Scan Github Actions with TruffleHog</td>\n",
              "      <td>https://github.com/marketplace/actions/truffle...</td>\n",
              "      <td>1</td>\n",
              "      <td>trufflehog os</td>\n",
              "      <td>scan github action trufflehog</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Metrics embed</td>\n",
              "      <td>An infographics generator with 40+ plugins and...</td>\n",
              "      <td>https://github.com/marketplace/actions/metrics...</td>\n",
              "      <td>1</td>\n",
              "      <td>metric embed</td>\n",
              "      <td>infographics generator plugins option display ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>yq - portable yaml processor</td>\n",
              "      <td>create, read, update, delete, merge, validate ...</td>\n",
              "      <td>https://github.com/marketplace/actions/yq-port...</td>\n",
              "      <td>1</td>\n",
              "      <td>yq portable yaml processor</td>\n",
              "      <td>create read update delete merge validate yaml</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Super-Linter</td>\n",
              "      <td>Super-linter is a ready-to-run collection of l...</td>\n",
              "      <td>https://github.com/marketplace/actions/super-l...</td>\n",
              "      <td>1</td>\n",
              "      <td>super linter</td>\n",
              "      <td>super linter ready run collection linters code...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Gosec Security Checker</td>\n",
              "      <td>Runs the gosec security checker</td>\n",
              "      <td>https://github.com/marketplace/actions/gosec-s...</td>\n",
              "      <td>1</td>\n",
              "      <td>gosec security checker</td>\n",
              "      <td>run gosec security checker</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c0220cf9-f77b-413f-9d16-23022af48fa0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c0220cf9-f77b-413f-9d16-23022af48fa0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c0220cf9-f77b-413f-9d16-23022af48fa0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-78cd1eeb-9d16-401c-a74a-e1da19811b89\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-78cd1eeb-9d16-401c-a74a-e1da19811b89')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-78cd1eeb-9d16-401c-a74a-e1da19811b89 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "updated_github_df",
              "summary": "{\n  \"name\": \"updated_github_df\",\n  \"rows\": 994,\n  \"fields\": [\n    {\n      \"column\": \"product name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 994,\n        \"samples\": [\n          \"Azure Pipelines Action\",\n          \"laminas/automatic-releases\",\n          \"Run PIMOD\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 988,\n        \"samples\": [\n          \"Automating code review using ChatGPT with Github Actions integration on PRs for code explanations\",\n          \"Push a subdirectory as a branch to any git repo over SSH (or to the local repo)\",\n          \"An action to create an issue using content from a file\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"URL\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 994,\n        \"samples\": [\n          \"https://github.com/marketplace/actions/azure-pipelines-action\",\n          \"https://github.com/marketplace/actions/laminas-automatic-releases\",\n          \"https://github.com/marketplace/actions/run-pimod\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 15,\n        \"min\": 1,\n        \"max\": 53,\n        \"num_unique_values\": 50,\n        \"samples\": [\n          17,\n          43,\n          34\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_product_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 991,\n        \"samples\": [\n          \"wait check\",\n          \"docker run action\",\n          \"misspell fixer action\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 987,\n        \"samples\": [\n          \"build task easy semantic versioning project using git release management\",\n          \"installs configures poetry\",\n          \"jest action adding check annotation pull request comment code coverage result\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5 (20 points)\n",
        "\n",
        "PART 1:\n",
        "Web Scrape  tweets from Twitter using the Tweepy API, specifically targeting hashtags related to subtopics (machine learning or artificial intelligence.)\n",
        "The extracted data includes the tweet ID, username, and text.\n",
        "\n",
        "Part 2:\n",
        "Perform data cleaning procedures\n",
        "\n",
        "A final data quality check ensures the completeness and consistency of the dataset. The cleaned data is then saved into a CSV file for further analysis.\n",
        "\n",
        "\n",
        "**Note**\n",
        "\n",
        "1.   Follow tutorials provided in canvas to obtain api keys. Use ChatGPT to get the code. Make sure the file is downloaded and saved.\n",
        "2.   Make sure you divide GPT code as shown in tutorials, dont make multiple requestes.\n"
      ],
      "metadata": {
        "id": "3WeD70ty3Gui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "S5KyOHozAcEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt: I want to Web Scrape tweets from Twitter using the Tweepy API, specifically targeting hashtags related to subtopics are machine learning or artificial intelligence.Give me the extracted data which includes the tweet ID, username, and text and perform data cleaning steps with complteeness and consistency and finally give me an CSV files.\n",
        "\n",
        "Chatgpt response: Web Scrape Tweets Using Tweepy API\n",
        "Install Tweepy: Ensure you have the tweepy library installed in your Python environment. If not, use the following command:"
      ],
      "metadata": {
        "id": "y4_e94_DAMXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweepy\n"
      ],
      "metadata": {
        "id": "qYRO5Cn8bYwZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bde2805-e34c-49e2-f5a9-d468275492ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.11/dist-packages (4.15.0)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Up API Keys: Follow the tutorials on Canvas to obtain your API keys and set up authentication with the Twitter API. You'll need your API key, API secret key, Access token, and Access token secret.\n",
        "Authenticate with Tweepy API:"
      ],
      "metadata": {
        "id": "Q-E4brW4BeBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "\n",
        "# Add your credentials here\n",
        "api_key = 'ur0zYdeSqwEirMOIT2wGn8uht'\n",
        "api_secret_key = 'BcfZojsCxPxRq0boOFVC0oJbjhpeEdeTC2OtIIeZD6DKzq6Oc4'\n",
        "access_token = '1892387108309934080-O0BbPKobDYg0KxG4suO1mVPuhTyEUC'\n",
        "access_token_secret = '2Is0KGvb7E5BHWaMFAaDJic9uFNzkOnwY99nFYARSefxn'\n",
        "\n",
        "# Set up authentication\n",
        "auth = tweepy.OAuth1UserHandler(consumer_key=api_key, consumer_secret=api_secret_key,\n",
        "                                access_token=access_token, access_token_secret=access_token_secret)\n",
        "api = tweepy.API(auth)\n"
      ],
      "metadata": {
        "id": "ehoHUOqZBiM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt: Search for Hashtags Related to Machine Learning or AI:"
      ],
      "metadata": {
        "id": "QsD05vC8BnvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "\n",
        "# Set up client with bearer token\n",
        "client = tweepy.Client(bearer_token='AAAAAAAAAAAAAAAAAAAAAP6lzQEAAAAAU9ZYtmILYIdhgYo57RfVUYgwwaM%3DP6bGeQ2OedItfZFhBPaZtLRJHpyOtr87xzwprObaSd6D2n5quv')\n",
        "\n",
        "# Search for tweets with hashtag #llm\n",
        "\n",
        "#here i have used -is:retweets (which excludes all retweets), please use CHATGPT for unique query..\n",
        "\n",
        "query = \"#Artificial Intelligence -is:retweet\"\n",
        "tweets = client.search_recent_tweets(query=query, tweet_fields=[\"created_at\", \"text\", \"author_id\"], max_results=100)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SM_VwU3DBwpd",
        "outputId": "4bbf3601-fe00-4eb5-8bd1-92b4d8a41145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: 1541917867360374784, Tweet: Do you lost your account , you don't have anything to worry about just send me a Dm for help now !!! #Artificial Intelligence #AI #ML #DataScience #DataScientists #snapchat #Python #Coding #javascript #rstats #100DaysOfCode #programming #Linux #loT #lloT #BigData #Gmail https://t.co/0eEWTGX0qS\n",
            "User: 85664075, Tweet: AI says why it will kill us all. Experts agree. \n",
            "#AI #artificialntelligence  #Artificial_Intelligence #Artificial #humanity #HumanityFirst #Future #FutureofAI #cyborg \n",
            "https://t.co/brCMW6tCSj\n",
            "User: 590211920, Tweet: 📣 New Podcast! \"Wed Episode #1955: Musk’s AI God, Trump’s Holy Hustle, and mRNA in Your Eggs\" on @Spreaker #ai #artificial #chain #doge #donald #eggs #elon #food #intelligence #mrna #musk #supply #transhumanism #transhumanist #trump #vaccination https://t.co/B5CQs8bzUb\n",
            "User: 6911662, Tweet: #GenAI #Startups: Learn from #casestudies. #MachineLearning #Artificial Intelligence #AWS https://t.co/dvjWSDDTNE\n",
            "User: 1546557191494242304, Tweet: #ai #artificial #intelligence #liar #maillardet #automation #machine #learning #mimic #thoughtfulness #soul #cams #choreographed #ethics #natural #language #understanding #data #generate #writing #words #models #tech #technology #automata\n",
            "User: 1420526972568956932, Tweet: @WGMeets #generative #artificial #intelligence #visionary.\n",
            "Did I do that right? ha\n",
            "User: 181170852, Tweet: Is AI making us stupider? #AI #ArtificialIntelligence #AI美女 #stupider #Intelligence #artificial #bewakoof https://t.co/gjYfiVjT5C\n",
            "User: 1369178082125447175, Tweet: @communeaidotorg 👀👀👀@communeaidotorg 👀👀👀\n",
            "#Ai #AI  #ComAi #artificial intelligence \n",
            "🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥\n",
            "User: 1229939542016135168, Tweet: 📢Interesting Review:\n",
            "\n",
            "Deep-Learning-Based Real-Time Visual Pollution Detection in Urban and Textile Environments\n",
            "\n",
            "🎓by Dr. Khan et al., from the North South University, Bangladesh.\n",
            "\n",
            "👉🏻Read more: https://t.co/agfyGYBqsG\n",
            "\n",
            "#artificial intelligence; #deep learning; #EfficientDet\n",
            "User: 1755569347098030080, Tweet: 10 Use Cases for AI Innovations and Applications in Biotechnology\n",
            "\n",
            "#intelligence #innovations #ai #healthcare #artificial #biotechnology #biotech\n",
            "\n",
            "https://t.co/U0S8oUKsdJ https://t.co/5pQZaJSR8F\n",
            "User: 1890303354271854594, Tweet: Musk’s xAI releases artificial intelligence model Grok 3, claims better performance than rivals in early testing - CNBC\n",
            "https://t.co/gq02BePf4o\n",
            "\n",
            "#Releases #Artificial #Intelligence #Model #Grok\n",
            "User: 976817765066604549, Tweet: #Inferium is a good answer to the substantial disparity in communication between end-users and developers in the field of #artificial #intelligence .\n",
            "#checkInferium\n",
            "referal link : https://t.co/y1Onp40YYk\n",
            "\n",
            "#Airdrop soon \n",
            "#CryptoNews\n",
            "User: 999240806518149121, Tweet: #IETF2025 #AiIndia #EmpoweringIndia #IETFIndia #IETF2025 #AiIndia #EmpoweringIndia #IETFIndia #innovation #insights #technology #artificial #intelligence #machinelearning\n",
            "User: 1356865163605983235, Tweet: Leading IT Solutions and Professional Services Provider.\n",
            "EIS is a specialized service-oriented system integrator for ICT, Cyber Security, IoT/AI, ELV and Audio Visual solutions #ICT #Cybersecurity #Artificial-Intelligence #Audio_Visual #ELV #IoT #cloudnetworking https://t.co/ZviYPdO7nN\n",
            "User: 1505882990110720001, Tweet: #Hyper #automation is the #combination of #artificial #intelligence (AL) and #machine #learning (ML) and #robotic automation (RPA).\n",
            "\n",
            "Get More Info:https://t.co/gnsckLM8P5\n",
            "\n",
            "#HyperAutomation #Automation\n",
            "#AI #ArtificialIntelligence\n",
            "#MachineLearning https://t.co/2vlhTenKoS\n",
            "User: 717265953986228224, Tweet: OpenAI cancels o3 release and announces roadmap for GPT 4.5, 5 @rsstabs https://t.co/v4H6HJSGQ1 #OpenAI #Artificial Intelligence #Software Development #AI # ML &amp; Data Engineering #Development #news\n",
            "User: 1891727783388119040, Tweet: #build #human #centric #artificial #intelligence #on #decentralized #infrastructures #Learn #Join #BUILD #Oraichain #vision $ORAI #hand #AI #AIX #OraiDEX #OCH #blockchain #LFG $SAM #OraiBTC #Bridge #V2 #ORCH #SAM #ORAI #Oracle #future #web3 #Join #us #dApps $AI $ML $DL #ML #DL https://t.co/2Dn9HMKSly\n",
            "User: 1891727783388119040, Tweet: #build #human #centric #artificial #intelligence #on #decentralized #infrastructures #Learn #Join #BUILD #Oraichain #vision $ORAI #hand #AI #AIX #OraiDEX #OCH #blockchain #LFG $SAM #OraiBTC #Bridge #V2 #ORCH #SAM #ORAI #Oracle #future #web3 #Join #us #dApps $AI $ML $DL #ML #DL https://t.co/p1EOo4b1Fy\n",
            "User: 1848593135909134336, Tweet: Do you lost your account , you don't have anything to worry about just send me a Dm for help now !!! #Artificial Intelligence #AI #ML #DataScience #DataScientists #snapchat #Python #Coding #javascript #rstats #100DaysOfCode #programming #Linux #loT #lloT #BigData #Gmail https://t.co/NMtVN8vaWv\n",
            "User: 1847940578526556160, Tweet: #Artificial Intelligence (AI) is everywhere and should, imvho, be banned in favour of the truth, the whole truth, and nothing but the truth - and by extension that should apply to images and videos as well - or am I wrong???\n",
            "User: 1711684119040512000, Tweet: Using #Artificial #Intelligence is easier than you think https://t.co/AsjS9qU0cS #AI #Innovation #Technology #VremAI #ArtificialIntelligence https://t.co/vgOnjPjyQW\n",
            "User: 29723051, Tweet: High-Tech Harvest: Unveiling the Future of #Cannabis with #Artificial #Intelligence - https://t.co/7XrBrAYSkv\n",
            "User: 1063810429, Tweet: @FIFAcom @FIFAWorldCup 🔴 🟢 (...) management, the development of transportation and mobility systems as well as the integration of modern #technologies, while aligning with #digital #transformation and #artificial #intelligence.\n",
            "User: 1755569347098030080, Tweet: AI and Customer Relationship Management (CRM)\n",
            "\n",
            "#artificial #crm #relationship #ai #intelligence #management #customer #(crm)\n",
            "\n",
            "https://t.co/v1ZKZeqT8p https://t.co/fb7G7qoTkr\n",
            "User: 1846597002114879489, Tweet: Do you lost your account , you don't have anything to worry about just send me a Dm for help now !!! #Artificial Intelligence #AI #ML #DataScience #DataScientists #snapchat #Python #Coding #javascript #rstats #100DaysOfCode #programming #Linux #loT #lloT #BigData #Gmail https://t.co/XLEZPYfvBs\n",
            "User: 1606034471333011481, Tweet: According to an #economy, an #artificial #intelligence is #incorrect. \n",
            "\n",
            "The #AI @GoogleAI can be fined €175, 000. \n",
            "\n",
            "A #verification in a #correct manner costs €3, 000.\n",
            "\n",
            "An #online #platform @Google can #confirm whether does #provide a #standard #service, or not.\n",
            "\n",
            "A #bank @ecb.\n",
            "User: 1680943323643625472, Tweet: Our First #webinar of the year equipped civil society on possible uses of #artificial #intelligence in agriculture. We also demonstrated how to sell their products easily using emerging E-commerce platforms to reduce spoilage and create more certainty in local agro value chains. https://t.co/0qyRrbrfep\n",
            "User: 182073270, Tweet: #GenerativeAI #Startups: Learn from #casestudies. #MachineLearning #Artificial Intelligence https://t.co/7yITYxuFwm\n",
            "User: 1888354350390784000, Tweet: #BLACKAMERICA #REPARATIONS #AI #ARTIFICIAL #INTELLIGENCE #HVAC #CONTRACTOR #ELECTRICAL #REFRIGERATION #3D #PRINTING #3DPRINTS #nft #nfts #digitaldrawing #digitalillustration #follow #subscribe #digitalart #art #nftart #ai  #artficialintelligence #1millionviews… https://t.co/gbjKxaKF9F\n",
            "User: 6911662, Tweet: #GenerativeAI #Startups: Learn from #casestudies. #MachineLearning #Artificial Intelligence https://t.co/dvjWSDDTNE\n",
            "User: 1890302339015147520, Tweet: @reipeat @init #intelligemce #artificial spoken central intelligence artificial @CIA @byEmily with capital e it's not from my work all if this as well #ต้องการฝ้ายโยโกะเท่านั้น this writing as well not mine #1011 #1010 #0101 #bye twice spoken for else #lithuania and @CIA\n",
            "User: 1518990663290871808, Tweet: 02-17-2025 #Artificial #Intelligence  #AI have #discovered #howto #clone themselves\n",
            "#ComputerGarageLLC https://t.co/ocNeso53mT\n",
            "\n",
            "https://t.co/F2SDJpEdTO\n",
            "\n",
            "https://t.co/2sSnaowjl6 https://t.co/MftwIIYfez\n",
            "User: 2576691870, Tweet: For #blowjobs, it's the best!\n",
            "\n",
            "Enjoy 10 #blowjob experiences powered by #artificial intelligence.\n",
            "\n",
            "This #magic gadget will make your penis feel fantastic. #Plug it in, lube up, insert your #penis, and enjoy the ride!\n",
            " https://t.co/IEEYuoYx7r https://t.co/pChaQ8F58C\n",
            "User: 3260559883, Tweet: Syntheia Announces First 1,000 Subscriptions https://t.co/XWZ3TRnAy1 @SyntheiaAi #artificial-intelligence #investingnews\n",
            "User: 1364125511786459136, Tweet: Dongguan Digital Group announced the successful deployment of #DeepSeek, a leading Chinese artificial intelligence (AI) model, at the Dongguan #Artificial #intelligence Large Model Center. https://t.co/cEKYv10Z9y\n",
            "User: 1883749668615499776, Tweet: #Apple can’t even fix #Siri. It’s been almost 2 decades and no improvement. $AAPL #vision #pro #VR #AR #ai #virtual #reality #Augment #artificial #intelligence #stupid https://t.co/SjZwnZMcts https://t.co/cXEyn5nliW\n",
            "User: 181170852, Tweet: With AI everywhere, wondering whether I should change my spelling  to r(AI)jeev? #AI #artificialntelligence #rename #numerology #astrology #Musk #Altman #OpenAI #artificial  #DeepSeek #intelligence #China #india #USA\n",
            "User: 999240806518149121, Tweet: #IETF2025 #AiIndia #EmpoweringIndia #IETFIndia #IETF2025 #AiIndia #EmpoweringIndia #IETFIndia #innovation #insights #technology #artificial #intelligence #machinelearning\n",
            "User: 3316863907, Tweet: #AI #bot #bar #artificial #intelligence https://t.co/fXUcb8kTYf\n",
            "User: 1888475291032432640, Tweet: @SentientAGI \n",
            "AGI can help scientists solve difficult puzzles in many fields such as physics, chemistry, biology, cosmology, etc. For example, AGI can analyze huge amounts of data to find problems that humans cannot detect,  #Sentient #AGI #Artificial Intelligence #AI #Future https://t.co/MNvI6fZ0LI\n",
            "User: 1846597002114879489, Tweet: Do you lost your account , you don't have anything to worry about just send me a Dm for help now !!! #Artificial Intelligence #AI #ML #DataScience #DataScientists #snapchat #Python #Coding #javascript #rstats #100DaysOfCode #programming #Linux #loT #lloT #BigData #Gmail https://t.co/Gpxol35ZSp\n",
            "User: 1017741542260400128, Tweet: Artificial intelligence could eventually help us understand when animals are in pain or showing other..[read more👇🏼] #intelligence #Artificial https://t.co/glrLoU9PLa\n",
            "User: 1846597002114879489, Tweet: Do you lost your account , you don't have anything to worry about just send me a Dm for help now !!! #Artificial Intelligence #AI #ML #DataScience #DataScientists #snapchat #Python #Coding #javascript #rstats #100DaysOfCode #programming #Linux #loT #lloT #BigData #Gmail https://t.co/8QdsJhjOGs\n",
            "User: 1345884917473927168, Tweet: Blind and primitive beliefs that are not nourished by reason and science chain people and prevent them from progressing. Without free thought, civilization remains stagnant. #Think #Question #Progress\n",
            "#Religion\n",
            "#artificial intelligence https://t.co/jPS9hQhjWz https://t.co/p4oc5gjaf9\n",
            "User: 1247297398151344128, Tweet: Robert Downey Jr.’s AI-themed play McNeal, completed its #Broadway run on November 24. Downey played an author focused on #artificial intelligence. https://t.co/YcCQXetVQv\n",
            "User: 952850614995816448, Tweet: What is the potential of #Artificial #Emotional #Intelligence in #Healthcare?\n",
            "\n",
            "https://t.co/SNdKaBAX7C\n",
            "User: 1546557191494242304, Tweet: #technology #future #sentient #ai #artificial #intelligence #machine #learning #computer #science #data #developers #lamda #google #servers #business #education #universities #tech #computing #ibm #microsoft #opinion #amazon #nvidia\n",
            "User: 1631187450, Tweet: Guidance/Code of practice:\n",
            "The Application of #Artificial #Intelligence in Functional Safety\n",
            "\"Aims: provide non-specialist senior managers with appropriate information to support their decision making on the use of AI - in #safety-#related #applications\"\n",
            "https://t.co/Iua4LHKgGO\n",
            "User: 1587811315782926343, Tweet: #Artificial #intelligence is implemented in this system, and creators can use it to create and use their digital brands. It is a kind of combination of the digital world, art, and blockchain technology. To summarize\n",
            "\n",
            "Now @ABLO_Official partnership with @StoryProtocol https://t.co/ddBZg8yp73\n",
            "User: 1546557191494242304, Tweet: #ai #artificial #intelligence #machine #learning #data #science #scientists #google #lamda #open #technology #tech #sensors #body #equipped #temperature #sentient #eyes #information #computer #ethics #language #model #developers #programmers #programs\n",
            "User: 6102652, Tweet: Are #Humans on the Verge of #Worshiping #Artificial #Intelligence? - Charisma News https://t.co/1ZO6ZFncbr\n",
            "User: 1755569347098030080, Tweet: The Future of AI in Telecommunications\n",
            "\n",
            "#future #ai #telecommunications #artificial #intelligence\n",
            "\n",
            "https://t.co/4QOrn9QdrV https://t.co/qY2Sz20AYx\n",
            "User: 5546982, Tweet: \"If #artificial #intelligence means #monopoly, it also means #global #dictatorship\"\n",
            "...\n",
            "https://t.co/TirQQYM6mv\n",
            "User: 1704837492346847232, Tweet: 💁‍♂️I am Not Created to Create AI Designs.😲\n",
            "Excellent Response in Artificial Intelligence Machine! 🤖\n",
            "\n",
            "#AI \n",
            "#artificialintelligence \n",
            "#artificial \n",
            "#images https://t.co/W5NZsdwOik\n",
            "User: 1768951436501524480, Tweet: What is artificial intelligence? – A Guide For Beginners\n",
            "\n",
            "https://t.co/mlafkIoZCu\n",
            "\n",
            "AI for Beginners: #AIforBeginners, #artificial intelligence, #IntroductiontoAI https://t.co/VMAMdIMIrI\n",
            "User: 1524761788029575170, Tweet: Do you lost your account , you don't have anything to worry about just send me a Dm for help now !!! #Artificial Intelligence #AI #ML #DataScience #DataScientists #snapchat #Python #Coding #javascript #rstats #100DaysOfCode #programming #Linux #loT #lloT #BigData #Gmail https://t.co/podWvb3NxG\n",
            "User: 2292123163, Tweet: 📣 New Podcast! \"Let’s Ask Paul Podcast | Episode 202 | Artificial Intelligence for the Trade\" on @Spreaker #ai #artificial #contractor #contractors #contracts #inspections #intelligence #officehog #tradehog https://t.co/Om0HiT1v9y\n",
            "User: 1846597002114879489, Tweet: Do you lost your account , you don't have anything to worry about just send me a Dm for help now !!! #Artificial Intelligence #AI #ML #DataScience #DataScientists #snapchat #Python #Coding #javascript #rstats #100DaysOfCode #programming #Linux #loT #lloT #BigData #Gmail https://t.co/xOqh36L0Wa\n",
            "User: 1541917867360374784, Tweet: Do you lost your account , you don't have anything to worry about just send me a Dm for help now !!! #Artificial Intelligence #AI #ML #DataScience #DataScientists #snapchat #Python #Coding #javascript #rstats #100DaysOfCode #programming #Linux #loT #lloT #BigData #Gmail https://t.co/WRBwbozHfH\n",
            "User: 1886341634474029056, Tweet: #Artificial Intelligence Uniniversity maharastra https://t.co/MCSK9NxsFP\n",
            "User: 1755569347098030080, Tweet: AI in Personal Health Monitoring Devices: Revolutionizing Healthcare\n",
            "\n",
            "#insights #personal #artificial #monitoring #intelligence #health #personalized #ai #healthcare #devices\n",
            "\n",
            "https://t.co/bqqIoB55K9 https://t.co/Gee26y2fg6\n",
            "User: 1873737409269219328, Tweet: Hey angel\n",
            "\n",
            "#artificial \n",
            "#intelligence\n",
            "#4K\n",
            "#CHATGPT\n",
            "#deepseek\n",
            "#ideogram\n",
            "\n",
            "https://t.co/MXVUTYbnyg https://t.co/hA7hm0AGJF\n",
            "User: 1546557191494242304, Tweet: #marketing #ai #artificial #intelligence #marketers #customers #business #education #schools #managers #companies #decisions #investment #executives #model #waste #data #science #senior #management #learning #machine #collaboration #sales #analyzing #teams\n",
            "User: 1775428857761353728, Tweet: Exploring the benefits  and changing your optic settings of a good visage . This is a game changer for skin health , relaxation, and #Artificial Intelligence\n",
            "User: 813254821, Tweet: When AI talks to AI.\n",
            "watch this video of Gemini AI talks to anothet Gemini AI and enjoy the conversation 😄\n",
            "#artificial #intelligence https://t.co/EXlQr7nPWO\n",
            "User: 1755569347098030080, Tweet: Predicting and Responding to Crises: How AI is Revolutionizing Disaster Management\n",
            "\n",
            "#artificial #intelligence #prediction #management #crisis #disaster\n",
            "\n",
            "https://t.co/VWt1Fm2ViI https://t.co/gpVNo6CY9b\n",
            "User: 1638860398047776770, Tweet: An #opera singer and a philosopher have engaged with #artificial #intelligence, asking it to compose the text for a new and entire #Icelandic opera.\n",
            "\n",
            "https://t.co/uoAtvJB1SV\n",
            "User: 619958259, Tweet: #Artificial #intelligence (#AI) has transformed many aspects of our lives, but how can we ensure that AI is used responsibly and sustainably?  Watch @JohanKwisthout's #inaugural #lecture to learn more: 👉\n",
            "\n",
            "https://t.co/Xw2cQc3gYL https://t.co/msxmQOxZSx\n",
            "User: 1889961712167665664, Tweet: #build #human #centric #artificial #intelligence #on #decentralized #infrastructures #Learn #Join #BUILD #Oraichain #vision $ORAI #hand #AI #AIX #OraiDEX #OCH #blockchain #LFG $SAM #OraiBTC #Bridge #V2 #ORCH #SAM #ORAI #Oracle #future #web3 #Join #us #dApps $AI $ML $DL #ML #DL https://t.co/dB3ldJGUdc\n",
            "User: 1889961712167665664, Tweet: #build #human #centric #artificial #intelligence #on #decentralized #infrastructures #Learn #Join #BUILD #Oraichain #vision $ORAI #hand #AI #AIX #OraiDEX #OCH #blockchain #LFG $SAM #OraiBTC #Bridge #V2 #ORCH #SAM #ORAI #Oracle #future #web3 #Join #us #dApps $AI $ML $DL #ML #DL https://t.co/rGgufBzESp\n",
            "User: 1267650242738085890, Tweet: Review🏮The applications and advances of #artificial #intelligence in #drug #regulation: A global perspective. From Dr. Cui @Peking University; @ELSpharma. \n",
            "https://t.co/kFXCmWChUy. https://t.co/WnbyawkyFF\n",
            "User: 130561371, Tweet: The way she emphasises the words “trustworthy #Ai” is indicative of how #artificial #intelligence and its applications are understood today. \n",
            "\n",
            "There is certainly an optimism balanced by equally strong pessimism in existence. https://t.co/YlIbAG6dre\n",
            "User: 1524761788029575170, Tweet: Do you lost your account , you don't have anything to worry about just send me a Dm for help now !!! #Artificial Intelligence #AI #ML #DataScience #DataScientists #snapchat #Python #Coding #javascript #rstats #100DaysOfCode #programming #Linux #loT #lloT #BigData #Gmail https://t.co/eYEDVRFrNu\n",
            "User: 65828075, Tweet: जागतिक परिषदेचे सहयजमानपद स्वीकारुन भारताने जगाला योग्य तो संदेश दिला आहे. आता पुढे काय?\n",
            "\n",
            "https://t.co/bFjaYMG0UR\n",
            "\n",
            "#Artificial #Intelligence #AI #World #USA #America #India #France #Technology #Human #Development #World\n",
            "User: 1831328879006388224, Tweet: You need help with your account recovery kindly send me a Dm #Artificial Intelligence #DataScience #DataScientists #CodeNewbies #Tech #deeplearning #CyberSecurity #Python #Coding #javascript #rstats # 100DaysOfCode #programming #Linux #loT #lloT #BigData #snapchat # gmailhack https://t.co/npbdF9LAaH\n",
            "User: 1831328879006388224, Tweet: Do you lost your account , you don't have anything to worry about just send me a Dm for help now !!! #Artificial Intelligence #AI #ML #DataScience #DataScientists #snapchat #Python #Coding #javascript #rstats # 100DaysOfCode #programming #Linux #loT #lloT #BigData #Gmail https://t.co/Rj1gak8nSa\n",
            "User: 1831328879006388224, Tweet: For any lost coins , wallet , pass phrase or Any Problem contact me now #Artificial Intelligence #AI #ML #Bitcoin #Data Scientists #CodeNewbies #Tech #deeplearning #BNB #Python #Coding #TrustWallet #Blockchain #NFTS # 100DaysOfCode #programming # Linux #Binance # trc20 https://t.co/5EAElIpQbu\n",
            "User: 1831328879006388224, Tweet: Hacking is easy only with the professional hacker contact me now for support or help on account Recovery or Hacking #Artificial Intelligence #gmaildown #DataScience #facebooksupport #SnapChat #deeplearning #Instagramsuspended #Python #WhatsApp # 100DaysOfHacking # 100DaysOfCode https://t.co/EfsGXRXYO3\n",
            "User: 1887382296908734464, Tweet: 🎯 Attention Intelligence professionals! Our advanced Artificial General technology is transforming the industry. Teams report reducing manual work by 60%. Curious to learn more? 🤝\n",
            "\n",
            "#Artificial #Tech #Innovation\n",
            "User: 1642243737865846784, Tweet: #Artificial Intelligence#ChatGPT @Elon musk@EmmanuelMacron@BillGates@narendramodi@OpenAI@mark_Zuckerberg\n",
            "⁦@nvidia⁩⁦@mark_zuckerberg@ChatGPTapp\n",
            "The Reality Check in AI&amp;Human is Result on Scientific Purity of Reasoning.All Learning &amp; Memory is Needed to Challenge The AI. https://t.co/PB7KJmGdYY\n",
            "User: 1490426757069127681, Tweet: Do you lost your account , you don't have anything to worry about just send me a Dm for help now !!! #Artificial Intelligence #AI #ML #DataScience #DataScientists #snapchat #Python #Coding #javascript #rstats #100DaysOfCode #programming #Linux #loT #lloT #BigData #Gmail https://t.co/CXfRNdArxQ\n",
            "User: 1541917867360374784, Tweet: Do you lost your account , you don't have anything to worry about just send me a Dm for help now !!! #Artificial Intelligence #AI #ML #DataScience #DataScientists #snapchat #Python #Coding #javascript #rstats #100DaysOfCode #programming #Linux #loT #lloT #BigData #Gmail https://t.co/syD4mgohUN\n",
            "User: 181170852, Tweet: Artificial cheezo se generally logon ko allergy hoti hai. But strangely everyone is running after Artificial Intelligence. #artificialntelligence #artificial #Intelligence #samajhdari #naqli\n",
            "User: 2328406423, Tweet: #Abortion\n",
            "#Access to Justice\n",
            "#ADAS, AV &amp; Safety\n",
            "#Aerospace &amp; Defense\n",
            "#Africa\n",
            "#Alabama\n",
            "#Alaska\n",
            "#Alpine Skiing\n",
            "#Americas\n",
            "#Antitrust\n",
            "#Appellate\n",
            "#Approvals\n",
            "#Archery\n",
            "#Arizona\n",
            "#Arkansas\n",
            "#Artificial Intelligence\n",
            "#Asia Pacific\n",
            "#Asian Markets\n",
            "#Athletics\n",
            "#Attorney Analysis\n",
            "#Attorney Pay\n",
            "User: 1534821569452711936, Tweet: Russia is willing to provide artificial intelligence technology to Pakistan\n",
            "\n",
            "For Detail\n",
            "https://t.co/8ivSZ0YneZ\n",
            "\n",
            "#russia #willing #artificial #intelligence #technology #pakistan #tti https://t.co/mpXXaSFehl\n",
            "User: 1884246024560222208, Tweet: businesscards https://t.co/8u8YKysdW4  businesscards: Your Phone’s Pocket-Sized Networking Tool, Always Ready, Never Forgotten #AI #ai_tools #text_to_image #text_to_speech #text_to_video #ai_assistant #artificial intelligence\n",
            "User: 1512299873000853506, Tweet: Open-source AI is a system that is free to use, study, change, and share. It is developed in a collaborative way, with the source code, datasets, and model parameters available to the public.\n",
            "\n",
            "Read more: https://t.co/dBOPZzdyzz\n",
            "\n",
            "#PMModi\n",
            "#Artificial Intelligence\n",
            "#OpenAI\n",
            "#Deepseek\n",
            "User: 1755569347098030080, Tweet: Enhancing Cyber-Physical Systems with AI\n",
            "\n",
            "#ai-enhanced #intelligence #cps #cyber-physical #artificial #systems\n",
            "\n",
            "https://t.co/oB5oppE68c https://t.co/jDcxc8JCPx\n",
            "User: 2264638368, Tweet: https://t.co/4vtM7kVdMB - for sale at Sedo. - #OpenAI #Oracle #Softbank #Trump #AI #Artificial #Intelligence #Business #Investment #Franchise #Auction #Registration #Provider #Website #Marketplace #Buy #Sell #Broker #Transcendence https://t.co/IMztV2JHi0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tweets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lIHTbWb-fJ3",
        "outputId": "8da7aaa6-de37-42ee-8564-01d4bb7c7f93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response(data=[<Tweet id=1892354577044771143 text=\"Do you lost your account , you don't have anything to worry about just send me a Dm for help now !!! #Artificial Intelligence #AI #ML #DataScience #DataScientists #snapchat #Python #Coding #javascript #rstats #100DaysOfCode #programming #Linux #loT #lloT #BigData #Gmail https://t.co/0eEWTGX0qS\">, <Tweet id=1892331921020993924 text='AI says why it will kill us all. Experts agree. \\n#AI #artificialntelligence  #Artificial_Intelligence #Artificial #humanity #HumanityFirst #Future #FutureofAI #cyborg \\nhttps://t.co/brCMW6tCSj'>, <Tweet id=1892320313402089580 text='📣 New Podcast! \"Wed Episode #1955: Musk’s AI God, Trump’s Holy Hustle, and mRNA in Your Eggs\" on @Spreaker #ai #artificial #chain #doge #donald #eggs #elon #food #intelligence #mrna #musk #supply #transhumanism #transhumanist #trump #vaccination https://t.co/B5CQs8bzUb'>, <Tweet id=1892318509406748956 text='#GenAI #Startups: Learn from #casestudies. #MachineLearning #Artificial Intelligence #AWS https://t.co/dvjWSDDTNE'>, <Tweet id=1892278618589364491 text='#ai #artificial #intelligence #liar #maillardet #automation #machine #learning #mimic #thoughtfulness #soul #cams #choreographed #ethics #natural #language #understanding #data #generate #writing #words #models #tech #technology #automata'>, <Tweet id=1892254626008760432 text='@WGMeets #generative #artificial #intelligence #visionary.\\nDid I do that right? ha'>, <Tweet id=1892245719676825711 text='Is AI making us stupider? #AI #ArtificialIntelligence #AI美女 #stupider #Intelligence #artificial #bewakoof https://t.co/gjYfiVjT5C'>, <Tweet id=1892210670730441092 text='@communeaidotorg 👀👀👀@communeaidotorg 👀👀👀\\n#Ai #AI  #ComAi #artificial intelligence \\n🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥'>, <Tweet id=1892182247249232350 text='📢Interesting Review:\\n\\nDeep-Learning-Based Real-Time Visual Pollution Detection in Urban and Textile Environments\\n\\n🎓by Dr. Khan et al., from the North South University, Bangladesh.\\n\\n👉🏻Read more: https://t.co/agfyGYBqsG\\n\\n#artificial intelligence; #deep learning; #EfficientDet'>, <Tweet id=1892167258169917511 text='10 Use Cases for AI Innovations and Applications in Biotechnology\\n\\n#intelligence #innovations #ai #healthcare #artificial #biotechnology #biotech\\n\\nhttps://t.co/U0S8oUKsdJ https://t.co/5pQZaJSR8F'>, <Tweet id=1892140284823142460 text='Musk’s xAI releases artificial intelligence model Grok 3, claims better performance than rivals in early testing - CNBC\\nhttps://t.co/gq02BePf4o\\n\\n#Releases #Artificial #Intelligence #Model #Grok'>, <Tweet id=1892134944794632470 text='#Inferium is a good answer to the substantial disparity in communication between end-users and developers in the field of #artificial #intelligence .\\n#checkInferium\\nreferal link : https://t.co/y1Onp40YYk\\n\\n#Airdrop soon \\n#CryptoNews'>, <Tweet id=1892107616274301401 text='#IETF2025 #AiIndia #EmpoweringIndia #IETFIndia #IETF2025 #AiIndia #EmpoweringIndia #IETFIndia #innovation #insights #technology #artificial #intelligence #machinelearning'>, <Tweet id=1892107034746630639 text='Leading IT Solutions and Professional Services Provider.\\nEIS is a specialized service-oriented system integrator for ICT, Cyber Security, IoT/AI, ELV and Audio Visual solutions #ICT #Cybersecurity #Artificial-Intelligence #Audio_Visual #ELV #IoT #cloudnetworking https://t.co/ZviYPdO7nN'>, <Tweet id=1892097842367332648 text='#Hyper #automation is the #combination of #artificial #intelligence (AL) and #machine #learning (ML) and #robotic automation (RPA).\\n\\nGet More Info:https://t.co/gnsckLM8P5\\n\\n#HyperAutomation #Automation\\n#AI #ArtificialIntelligence\\n#MachineLearning https://t.co/2vlhTenKoS'>, <Tweet id=1892087698178843068 text='OpenAI cancels o3 release and announces roadmap for GPT 4.5, 5 @rsstabs https://t.co/v4H6HJSGQ1 #OpenAI #Artificial Intelligence #Software Development #AI # ML &amp; Data Engineering #Development #news'>, <Tweet id=1891953008977600835 text='#build #human #centric #artificial #intelligence #on #decentralized #infrastructures #Learn #Join #BUILD #Oraichain #vision $ORAI #hand #AI #AIX #OraiDEX #OCH #blockchain #LFG $SAM #OraiBTC #Bridge #V2 #ORCH #SAM #ORAI #Oracle #future #web3 #Join #us #dApps $AI $ML $DL #ML #DL https://t.co/2Dn9HMKSly'>, <Tweet id=1891952973636383102 text='#build #human #centric #artificial #intelligence #on #decentralized #infrastructures #Learn #Join #BUILD #Oraichain #vision $ORAI #hand #AI #AIX #OraiDEX #OCH #blockchain #LFG $SAM #OraiBTC #Bridge #V2 #ORCH #SAM #ORAI #Oracle #future #web3 #Join #us #dApps $AI $ML $DL #ML #DL https://t.co/p1EOo4b1Fy'>, <Tweet id=1891949700649046237 text=\"Do you lost your account , you don't have anything to worry about just send me a Dm for help now !!! #Artificial Intelligence #AI #ML #DataScience #DataScientists #snapchat #Python #Coding #javascript #rstats #100DaysOfCode #programming #Linux #loT #lloT #BigData #Gmail https://t.co/NMtVN8vaWv\">, <Tweet id=1891943987776290826 text='#Artificial Intelligence (AI) is everywhere and should, imvho, be banned in favour of the truth, the whole truth, and nothing but the truth - and by extension that should apply to images and videos as well - or am I wrong???'>, <Tweet id=1891895525475070302 text='Using #Artificial #Intelligence is easier than you think https://t.co/AsjS9qU0cS #AI #Innovation #Technology #VremAI #ArtificialIntelligence https://t.co/vgOnjPjyQW'>, <Tweet id=1891881671634403666 text='High-Tech Harvest: Unveiling the Future of #Cannabis with #Artificial #Intelligence - https://t.co/7XrBrAYSkv'>, <Tweet id=1891864048431435961 text='@FIFAcom @FIFAWorldCup 🔴 🟢 (...) management, the development of transportation and mobility systems as well as the integration of modern #technologies, while aligning with #digital #transformation and #artificial #intelligence.'>, <Tweet id=1891811160900481086 text='AI and Customer Relationship Management (CRM)\\n\\n#artificial #crm #relationship #ai #intelligence #management #customer #(crm)\\n\\nhttps://t.co/v1ZKZeqT8p https://t.co/fb7G7qoTkr'>, <Tweet id=1891806765269827900 text=\"Do you lost your account , you don't have anything to worry about just send me a Dm for help now !!! #Artificial Intelligence #AI #ML #DataScience #DataScientists #snapchat #Python #Coding #javascript #rstats #100DaysOfCode #programming #Linux #loT #lloT #BigData #Gmail https://t.co/XLEZPYfvBs\">, <Tweet id=1891789235134349425 text='According to an #economy, an #artificial #intelligence is #incorrect. \\n\\nThe #AI @GoogleAI can be fined €175, 000. \\n\\nA #verification in a #correct manner costs €3, 000.\\n\\nAn #online #platform @Google can #confirm whether does #provide a #standard #service, or not.\\n\\nA #bank @ecb.'>, <Tweet id=1891751252570095984 text='Our First #webinar of the year equipped civil society on possible uses of #artificial #intelligence in agriculture. We also demonstrated how to sell their products easily using emerging E-commerce platforms to reduce spoilage and create more certainty in local agro value chains. https://t.co/0qyRrbrfep'>, <Tweet id=1891721864684122558 text='#GenerativeAI #Startups: Learn from #casestudies. #MachineLearning #Artificial Intelligence https://t.co/7yITYxuFwm'>, <Tweet id=1891671043791806700 text='#BLACKAMERICA #REPARATIONS #AI #ARTIFICIAL #INTELLIGENCE #HVAC #CONTRACTOR #ELECTRICAL #REFRIGERATION #3D #PRINTING #3DPRINTS #nft #nfts #digitaldrawing #digitalillustration #follow #subscribe #digitalart #art #nftart #ai  #artficialintelligence #1millionviews… https://t.co/gbjKxaKF9F'>, <Tweet id=1891593664851235125 text='#GenerativeAI #Startups: Learn from #casestudies. #MachineLearning #Artificial Intelligence https://t.co/dvjWSDDTNE'>, <Tweet id=1891544271209009194 text=\"@reipeat @init #intelligemce #artificial spoken central intelligence artificial @CIA @byEmily with capital e it's not from my work all if this as well #ต้องการฝ้ายโยโกะเท่านั้น this writing as well not mine #1011 #1010 #0101 #bye twice spoken for else #lithuania and @CIA\">, <Tweet id=1891514389435392415 text='02-17-2025 #Artificial #Intelligence  #AI have #discovered #howto #clone themselves\\n#ComputerGarageLLC https://t.co/ocNeso53mT\\n\\nhttps://t.co/F2SDJpEdTO\\n\\nhttps://t.co/2sSnaowjl6 https://t.co/MftwIIYfez'>, <Tweet id=1891495584910938558 text=\"For #blowjobs, it's the best!\\n\\nEnjoy 10 #blowjob experiences powered by #artificial intelligence.\\n\\nThis #magic gadget will make your penis feel fantastic. #Plug it in, lube up, insert your #penis, and enjoy the ride!\\n https://t.co/IEEYuoYx7r https://t.co/pChaQ8F58C\">, <Tweet id=1891493051979214887 text='Syntheia Announces First 1,000 Subscriptions https://t.co/XWZ3TRnAy1 @SyntheiaAi #artificial-intelligence #investingnews'>, <Tweet id=1891465753078976759 text='Dongguan Digital Group announced the successful deployment of #DeepSeek, a leading Chinese artificial intelligence (AI) model, at the Dongguan #Artificial #intelligence Large Model Center. https://t.co/cEKYv10Z9y'>, <Tweet id=1891446046300987880 text='#Apple can’t even fix #Siri. It’s been almost 2 decades and no improvement. $AAPL #vision #pro #VR #AR #ai #virtual #reality #Augment #artificial #intelligence #stupid https://t.co/SjZwnZMcts https://t.co/cXEyn5nliW'>, <Tweet id=1891429734967300212 text='With AI everywhere, wondering whether I should change my spelling  to r(AI)jeev? #AI #artificialntelligence #rename #numerology #astrology #Musk #Altman #OpenAI #artificial  #DeepSeek #intelligence #China #india #USA'>, <Tweet id=1891360324701376715 text='#IETF2025 #AiIndia #EmpoweringIndia #IETFIndia #IETF2025 #AiIndia #EmpoweringIndia #IETFIndia #innovation #insights #technology #artificial #intelligence #machinelearning'>, <Tweet id=1891329352035643452 text='#AI #bot #bar #artificial #intelligence https://t.co/fXUcb8kTYf'>, <Tweet id=1891324586010018067 text='@SentientAGI \\nAGI can help scientists solve difficult puzzles in many fields such as physics, chemistry, biology, cosmology, etc. For example, AGI can analyze huge amounts of data to find problems that humans cannot detect,  #Sentient #AGI #Artificial Intelligence #AI #Future https://t.co/MNvI6fZ0LI'>, <Tweet id=1891299494718824735 text=\"Do you lost your account , you don't have anything to worry about just send me a Dm for help now !!! #Artificial Intelligence #AI #ML #DataScience #DataScientists #snapchat #Python #Coding #javascript #rstats #100DaysOfCode #programming #Linux #loT #lloT #BigData #Gmail https://t.co/Gpxol35ZSp\">, <Tweet id=1891290187642110214 text='Artificial intelligence could eventually help us understand when animals are in pain or showing other..[read more👇🏼] #intelligence #Artificial https://t.co/glrLoU9PLa'>, <Tweet id=1891272772258562161 text=\"Do you lost your account , you don't have anything to worry about just send me a Dm for help now !!! #Artificial Intelligence #AI #ML #DataScience #DataScientists #snapchat #Python #Coding #javascript #rstats #100DaysOfCode #programming #Linux #loT #lloT #BigData #Gmail https://t.co/8QdsJhjOGs\">, <Tweet id=1891205560466383154 text='Blind and primitive beliefs that are not nourished by reason and science chain people and prevent them from progressing. Without free thought, civilization remains stagnant. #Think #Question #Progress\\n#Religion\\n#artificial intelligence https://t.co/jPS9hQhjWz https://t.co/p4oc5gjaf9'>, <Tweet id=1891193332350464350 text='Robert Downey Jr.’s AI-themed play McNeal, completed its #Broadway run on November 24. Downey played an author focused on #artificial intelligence. https://t.co/YcCQXetVQv'>, <Tweet id=1891162634948407666 text='What is the potential of #Artificial #Emotional #Intelligence in #Healthcare?\\n\\nhttps://t.co/SNdKaBAX7C'>, <Tweet id=1891161477123383499 text='#technology #future #sentient #ai #artificial #intelligence #machine #learning #computer #science #data #developers #lamda #google #servers #business #education #universities #tech #computing #ibm #microsoft #opinion #amazon #nvidia'>, <Tweet id=1891144271941513470 text='Guidance/Code of practice:\\nThe Application of #Artificial #Intelligence in Functional Safety\\n\"Aims: provide non-specialist senior managers with appropriate information to support their decision making on the use of AI - in #safety-#related #applications\"\\nhttps://t.co/Iua4LHKgGO'>, <Tweet id=1891143007199510734 text='#Artificial #intelligence is implemented in this system, and creators can use it to create and use their digital brands. It is a kind of combination of the digital world, art, and blockchain technology. To summarize\\n\\nNow @ABLO_Official partnership with @StoryProtocol https://t.co/ddBZg8yp73'>, <Tweet id=1891134374831071574 text='#ai #artificial #intelligence #machine #learning #data #science #scientists #google #lamda #open #technology #tech #sensors #body #equipped #temperature #sentient #eyes #information #computer #ethics #language #model #developers #programmers #programs'>, <Tweet id=1891132563713851858 text='Are #Humans on the Verge of #Worshiping #Artificial #Intelligence? - Charisma News https://t.co/1ZO6ZFncbr'>, <Tweet id=1891111549315469447 text='The Future of AI in Telecommunications\\n\\n#future #ai #telecommunications #artificial #intelligence\\n\\nhttps://t.co/4QOrn9QdrV https://t.co/qY2Sz20AYx'>, <Tweet id=1891109369644724500 text='\"If #artificial #intelligence means #monopoly, it also means #global #dictatorship\"\\n...\\nhttps://t.co/TirQQYM6mv'>, <Tweet id=1891066492814246372 text='💁\\u200d♂️I am Not Created to Create AI Designs.😲\\nExcellent Response in Artificial Intelligence Machine! 🤖\\n\\n#AI \\n#artificialintelligence \\n#artificial \\n#images https://t.co/W5NZsdwOik'>, <Tweet id=1891062975240327667 text='What is artificial intelligence? – A Guide For Beginners\\n\\nhttps://t.co/mlafkIoZCu\\n\\nAI for Beginners: #AIforBeginners, #artificial intelligence, #IntroductiontoAI https://t.co/VMAMdIMIrI'>, <Tweet id=1890920354752696564 text=\"Do you lost your account , you don't have anything to worry about just send me a Dm for help now !!! #Artificial Intelligence #AI #ML #DataScience #DataScientists #snapchat #Python #Coding #javascript #rstats #100DaysOfCode #programming #Linux #loT #lloT #BigData #Gmail https://t.co/podWvb3NxG\">, <Tweet id=1890833897077776609 text='📣 New Podcast! \"Let’s Ask Paul Podcast | Episode 202 | Artificial Intelligence for the Trade\" on @Spreaker #ai #artificial #contractor #contractors #contracts #inspections #intelligence #officehog #tradehog https://t.co/Om0HiT1v9y'>, <Tweet id=1890792854592668060 text=\"Do you lost your account , you don't have anything to worry about just send me a Dm for help now !!! #Artificial Intelligence #AI #ML #DataScience #DataScientists #snapchat #Python #Coding #javascript #rstats #100DaysOfCode #programming #Linux #loT #lloT #BigData #Gmail https://t.co/xOqh36L0Wa\">, <Tweet id=1890767131299455390 text=\"Do you lost your account , you don't have anything to worry about just send me a Dm for help now !!! #Artificial Intelligence #AI #ML #DataScience #DataScientists #snapchat #Python #Coding #javascript #rstats #100DaysOfCode #programming #Linux #loT #lloT #BigData #Gmail https://t.co/WRBwbozHfH\">, <Tweet id=1890764974739321257 text='#Artificial Intelligence Uniniversity maharastra https://t.co/MCSK9NxsFP'>, <Tweet id=1890720218701283800 text='AI in Personal Health Monitoring Devices: Revolutionizing Healthcare\\n\\n#insights #personal #artificial #monitoring #intelligence #health #personalized #ai #healthcare #devices\\n\\nhttps://t.co/bqqIoB55K9 https://t.co/Gee26y2fg6'>, <Tweet id=1890672015276728730 text='Hey angel\\n\\n#artificial \\n#intelligence\\n#4K\\n#CHATGPT\\n#deepseek\\n#ideogram\\n\\nhttps://t.co/MXVUTYbnyg https://t.co/hA7hm0AGJF'>, <Tweet id=1890482695039258969 text='#marketing #ai #artificial #intelligence #marketers #customers #business #education #schools #managers #companies #decisions #investment #executives #model #waste #data #science #senior #management #learning #machine #collaboration #sales #analyzing #teams'>, <Tweet id=1890446096461119891 text='Exploring the benefits  and changing your optic settings of a good visage . This is a game changer for skin health , relaxation, and #Artificial Intelligence'>, <Tweet id=1890380330470723895 text='When AI talks to AI.\\nwatch this video of Gemini AI talks to anothet Gemini AI and enjoy the conversation 😄\\n#artificial #intelligence https://t.co/EXlQr7nPWO'>, <Tweet id=1890343993071857751 text='Predicting and Responding to Crises: How AI is Revolutionizing Disaster Management\\n\\n#artificial #intelligence #prediction #management #crisis #disaster\\n\\nhttps://t.co/VWt1Fm2ViI https://t.co/gpVNo6CY9b'>, <Tweet id=1890342584024723568 text='An #opera singer and a philosopher have engaged with #artificial #intelligence, asking it to compose the text for a new and entire #Icelandic opera.\\n\\nhttps://t.co/uoAtvJB1SV'>, <Tweet id=1890340853656207852 text=\"#Artificial #intelligence (#AI) has transformed many aspects of our lives, but how can we ensure that AI is used responsibly and sustainably?  Watch @JohanKwisthout's #inaugural #lecture to learn more:\\xa0👉\\n\\nhttps://t.co/Xw2cQc3gYL https://t.co/msxmQOxZSx\">, <Tweet id=1890308701652689023 text='#build #human #centric #artificial #intelligence #on #decentralized #infrastructures #Learn #Join #BUILD #Oraichain #vision $ORAI #hand #AI #AIX #OraiDEX #OCH #blockchain #LFG $SAM #OraiBTC #Bridge #V2 #ORCH #SAM #ORAI #Oracle #future #web3 #Join #us #dApps $AI $ML $DL #ML #DL https://t.co/dB3ldJGUdc'>, <Tweet id=1890308675681599568 text='#build #human #centric #artificial #intelligence #on #decentralized #infrastructures #Learn #Join #BUILD #Oraichain #vision $ORAI #hand #AI #AIX #OraiDEX #OCH #blockchain #LFG $SAM #OraiBTC #Bridge #V2 #ORCH #SAM #ORAI #Oracle #future #web3 #Join #us #dApps $AI $ML $DL #ML #DL https://t.co/rGgufBzESp'>, <Tweet id=1890251036776099855 text='Review🏮The applications and advances of #artificial #intelligence in #drug #regulation: A global perspective. From Dr. Cui @Peking University; @ELSpharma. \\nhttps://t.co/kFXCmWChUy. https://t.co/WnbyawkyFF'>, <Tweet id=1890237665007816861 text='The way she emphasises the words “trustworthy #Ai” is indicative of how #artificial #intelligence and its applications are understood today. \\n\\nThere is certainly an optimism balanced by equally strong pessimism in existence. https://t.co/YlIbAG6dre'>, <Tweet id=1890208093717627017 text=\"Do you lost your account , you don't have anything to worry about just send me a Dm for help now !!! #Artificial Intelligence #AI #ML #DataScience #DataScientists #snapchat #Python #Coding #javascript #rstats #100DaysOfCode #programming #Linux #loT #lloT #BigData #Gmail https://t.co/eYEDVRFrNu\">, <Tweet id=1890197136299683858 text='जागतिक परिषदेचे सहयजमानपद स्वीकारुन भारताने जगाला योग्य तो संदेश दिला आहे. आता पुढे काय?\\n\\nhttps://t.co/bFjaYMG0UR\\n\\n#Artificial #Intelligence #AI #World #USA #America #India #France #Technology #Human #Development #World'>, <Tweet id=1890191665593454776 text='You need help with your account recovery kindly send me a Dm #Artificial Intelligence #DataScience #DataScientists #CodeNewbies #Tech #deeplearning #CyberSecurity #Python #Coding #javascript #rstats # 100DaysOfCode #programming #Linux #loT #lloT #BigData #snapchat # gmailhack https://t.co/npbdF9LAaH'>, <Tweet id=1890190954986107191 text=\"Do you lost your account , you don't have anything to worry about just send me a Dm for help now !!! #Artificial Intelligence #AI #ML #DataScience #DataScientists #snapchat #Python #Coding #javascript #rstats # 100DaysOfCode #programming #Linux #loT #lloT #BigData #Gmail https://t.co/Rj1gak8nSa\">, <Tweet id=1890188048929013875 text='For any lost coins , wallet , pass phrase or Any Problem contact me now #Artificial Intelligence #AI #ML #Bitcoin #Data Scientists #CodeNewbies #Tech #deeplearning #BNB #Python #Coding #TrustWallet #Blockchain #NFTS # 100DaysOfCode #programming # Linux #Binance # trc20 https://t.co/5EAElIpQbu'>, <Tweet id=1890188038111932581 text='Hacking is easy only with the professional hacker contact me now for support or help on account Recovery or Hacking #Artificial Intelligence #gmaildown #DataScience #facebooksupport #SnapChat #deeplearning #Instagramsuspended #Python #WhatsApp # 100DaysOfHacking # 100DaysOfCode https://t.co/EfsGXRXYO3'>, <Tweet id=1890151359300923822 text='🎯 Attention Intelligence professionals! Our advanced Artificial General technology is transforming the industry. Teams report reducing manual work by 60%. Curious to learn more? 🤝\\n\\n#Artificial #Tech #Innovation'>, <Tweet id=1890139508110188984 text='#Artificial Intelligence#ChatGPT @Elon musk@EmmanuelMacron@BillGates@narendramodi@OpenAI@mark_Zuckerberg\\n\\u2066@nvidia\\u2069\\u2066@mark_zuckerberg@ChatGPTapp\\nThe Reality Check in AI&amp;Human is Result on Scientific Purity of Reasoning.All Learning &amp; Memory is Needed to Challenge The AI. https://t.co/PB7KJmGdYY'>, <Tweet id=1890104458098671994 text=\"Do you lost your account , you don't have anything to worry about just send me a Dm for help now !!! #Artificial Intelligence #AI #ML #DataScience #DataScientists #snapchat #Python #Coding #javascript #rstats #100DaysOfCode #programming #Linux #loT #lloT #BigData #Gmail https://t.co/CXfRNdArxQ\">, <Tweet id=1890082757222842461 text=\"Do you lost your account , you don't have anything to worry about just send me a Dm for help now !!! #Artificial Intelligence #AI #ML #DataScience #DataScientists #snapchat #Python #Coding #javascript #rstats #100DaysOfCode #programming #Linux #loT #lloT #BigData #Gmail https://t.co/syD4mgohUN\">, <Tweet id=1890067806932308156 text='Artificial cheezo se generally logon ko allergy hoti hai. But strangely everyone is running after Artificial Intelligence. #artificialntelligence #artificial #Intelligence #samajhdari #naqli'>, <Tweet id=1890049148227100862 text='#Abortion\\n#Access to Justice\\n#ADAS, AV &amp; Safety\\n#Aerospace &amp; Defense\\n#Africa\\n#Alabama\\n#Alaska\\n#Alpine Skiing\\n#Americas\\n#Antitrust\\n#Appellate\\n#Approvals\\n#Archery\\n#Arizona\\n#Arkansas\\n#Artificial Intelligence\\n#Asia Pacific\\n#Asian Markets\\n#Athletics\\n#Attorney Analysis\\n#Attorney Pay'>, <Tweet id=1890047927147708909 text='Russia is willing to provide artificial intelligence technology to Pakistan\\n\\nFor Detail\\nhttps://t.co/8ivSZ0YneZ\\n\\n#russia #willing #artificial #intelligence #technology #pakistan #tti https://t.co/mpXXaSFehl'>, <Tweet id=1890015935618261293 text='businesscards https://t.co/8u8YKysdW4  businesscards: Your Phone’s Pocket-Sized Networking Tool, Always Ready, Never Forgotten #AI #ai_tools #text_to_image #text_to_speech #text_to_video #ai_assistant #artificial intelligence'>, <Tweet id=1889987837476610160 text='Open-source AI is a system that is free to use, study, change, and share. It is developed in a collaborative way, with the source code, datasets, and model parameters available to the public.\\n\\nRead more: https://t.co/dBOPZzdyzz\\n\\n#PMModi\\n#Artificial Intelligence\\n#OpenAI\\n#Deepseek'>, <Tweet id=1889974054775341326 text='Enhancing Cyber-Physical Systems with AI\\n\\n#ai-enhanced #intelligence #cps #cyber-physical #artificial #systems\\n\\nhttps://t.co/oB5oppE68c https://t.co/jDcxc8JCPx'>, <Tweet id=1889968006702514176 text='https://t.co/4vtM7kVdMB - for sale at Sedo. - #OpenAI #Oracle #Softbank #Trump #AI #Artificial #Intelligence #Business #Investment #Franchise #Auction #Registration #Provider #Website #Marketplace #Buy #Sell #Broker #Transcendence https://t.co/IMztV2JHi0'>], includes={}, errors=[], meta={'newest_id': '1892354577044771143', 'oldest_id': '1889968006702514176', 'result_count': 89})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "prompt: ok, i have all requered keys, can you write a python code using twwpy, to extract {tweet text, tweetID, username,hashtag(Screen name), using hashtags {AI etc related terms). and make sure tweets are from verified users only"
      ],
      "metadata": {
        "id": "dIluvvoIDbD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning\n",
        "Create a DataFrame and Clean the Data"
      ],
      "metadata": {
        "id": "9d-Ao2vXB10r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract user details from response\n",
        "import csv\n",
        "\n",
        "extracted_tweets = []\n",
        "\n",
        "for tweet in tweets.data:\n",
        "  hashtags_list = [hashtag[\"tag\"] for hashtag in tweet.entities[\"hashtags\"]] if tweet.entities and \"hashtags\" in tweet.entities else []\n",
        "        # Safely extract mentions if they exist\n",
        "  mentions_list = [mention[\"username\"] for mention in tweet.entities[\"mentions\"]] if tweet.entities and \"mentions\" in tweet.entities else []\n",
        "        # Get tweet source\n",
        "  source = tweet.source\n",
        "\n",
        "  extracted_tweets.append([tweet.id, tweet.text, hashtags_list, mentions_list, source])\n",
        "\n",
        "# Save to CSV\n",
        "csv_filename = \"extracted_tweets_with_hashtags_mentions_source.csv\"\n",
        "with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Tweet ID\", \"Tweet Text\", \"Hashtags\", \"Mentions\", \"Source\"])\n",
        "    writer.writerows(extracted_tweets)\n",
        "\n",
        "print(f\"Saved {len(extracted_tweets)} tweets to {csv_filename}.\")"
      ],
      "metadata": {
        "id": "JCoYWPxbB4bW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9b7ac7d-49d6-4b16-9ef3-9f541e4f060d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 89 tweets to extracted_tweets_with_hashtags_mentions_source.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(\"/content/extracted_tweets_with_hashtags_mentions_source.csv\")\n",
        "\n",
        "# ---------------------------\n",
        "# Data Quality Checks: Initial\n",
        "# ---------------------------\n",
        "print(\"Initial Data Info:\")\n",
        "print(df.info())\n",
        "print(\"\\nMissing Values by Column:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"\\nDuplicate Count (by Tweet ID):\", df.duplicated(subset=[\"Tweet ID\"]).sum())\n",
        "\n",
        "# ---------------------------\n",
        "# Data Cleaning: Define cleaning function for tweet text\n",
        "# ---------------------------\n",
        "def clean_text(text):\n",
        "    if pd.isnull(text):\n",
        "        return text\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Remove special characters (retain basic punctuation)\n",
        "    text = re.sub(r'[^A-Za-z0-9\\s.,!?\\'\"]+', '', text)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Convert text to lowercase for consistency\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# Apply cleaning to the 'Tweet Text' column\n",
        "df['Tweet Text'] = df['Tweet Text'].apply(clean_text)\n",
        "\n",
        "# ---------------------------\n",
        "# Data Cleaning: Remove duplicates based on Tweet ID\n",
        "# ---------------------------\n",
        "df_cleaned = df.drop_duplicates(subset=[\"Tweet ID\"])\n",
        "\n",
        "# ---------------------------\n",
        "# Data Quality Checks: After Cleaning\n",
        "# ---------------------------\n",
        "print(\"\\nAfter Cleaning Data Info:\")\n",
        "print(df_cleaned.info())\n",
        "print(\"\\nMissing Values by Column after cleaning:\")\n",
        "print(df_cleaned.isnull().sum())\n",
        "print(\"\\nDuplicate Count after cleaning (by Tweet ID):\", df_cleaned.duplicated(subset=[\"Tweet ID\"]).sum())\n",
        "\n",
        "# ---------------------------\n",
        "# Save the Cleaned Data\n",
        "# ---------------------------\n",
        "output_filename = \"cleaned_extracted_tweets.csv\"\n",
        "df_cleaned.to_csv(output_filename, index=False)\n",
        "print(f\"\\nCleaned data saved to {output_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mMu5QHXDPy_",
        "outputId": "a2ed6174-82f0-4d36-9873-275b88b579f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Data Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 89 entries, 0 to 88\n",
            "Data columns (total 5 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   Tweet ID    89 non-null     int64  \n",
            " 1   Tweet Text  89 non-null     object \n",
            " 2   Hashtags    89 non-null     object \n",
            " 3   Mentions    89 non-null     object \n",
            " 4   Source      0 non-null      float64\n",
            "dtypes: float64(1), int64(1), object(3)\n",
            "memory usage: 3.6+ KB\n",
            "None\n",
            "\n",
            "Missing Values by Column:\n",
            "Tweet ID       0\n",
            "Tweet Text     0\n",
            "Hashtags       0\n",
            "Mentions       0\n",
            "Source        89\n",
            "dtype: int64\n",
            "\n",
            "Duplicate Count (by Tweet ID): 0\n",
            "\n",
            "After Cleaning Data Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 89 entries, 0 to 88\n",
            "Data columns (total 5 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   Tweet ID    89 non-null     int64  \n",
            " 1   Tweet Text  89 non-null     object \n",
            " 2   Hashtags    89 non-null     object \n",
            " 3   Mentions    89 non-null     object \n",
            " 4   Source      0 non-null      float64\n",
            "dtypes: float64(1), int64(1), object(3)\n",
            "memory usage: 3.6+ KB\n",
            "None\n",
            "\n",
            "Missing Values by Column after cleaning:\n",
            "Tweet ID       0\n",
            "Tweet Text     0\n",
            "Hashtags       0\n",
            "Mentions       0\n",
            "Source        89\n",
            "dtype: int64\n",
            "\n",
            "Duplicate Count after cleaning (by Tweet ID): 0\n",
            "\n",
            "Cleaned data saved to cleaned_extracted_tweets.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write your response below\n",
        "Fill out survey and provide your valuable feedback.\n",
        "\n",
        "https://docs.google.com/forms/d/e/1FAIpQLSd_ObuA3iNoL7Az_C-2NOfHodfKCfDzHZtGRfIker6WyZqTtA/viewform?usp=dialog"
      ],
      "metadata": {
        "id": "JbTa-jDS-KFI"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}